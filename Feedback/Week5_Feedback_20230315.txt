Starting code feedback for Anqi, Week5

Current Points = 100

Note that: 
(1) Major sections begin with a double "====" line 
(2) Subsections begin with a single "====" line 
(3) Code output or text file content are printed within single "*****" lines 

======================================================================
======================================================================
Your Git repo size this week is about 337.88 MiB on disk 

PART 1: Checking project workflow...

Found the following directories in parent directory: .git, MiniProject, week6, week7, week4, week3, week2, Feedback, HPC, week1, week5

Found the following files in parent directory: README.md, .gitignore

Checking for key files in parent directory...

Found .gitignore in parent directory, great! 

Printing contents of .gitignore:

**********************************************************************
*.tmp
*.log
.DS_Store
*/sandbox
.vscode
**********************************************************************

Found README in parent directory, named: README.md

Printing contents of README.md:

**********************************************************************
# Anqi's CMEE Coursework Repository
This README file contains details about the modules within Anqi Wang's (aw222@ic.ac.uk) CMEE Coursework. This coursework contributes towards the fulfilment of MSc Computational Methods in Ecology and Evolution (CMEE) at Imperial College London. <br />

Many of the task requirements and information can be found at [The Mulitilingual Quantitative Biologist](https://mhasoba.github.io/TheMulQuaBio/intro.html)

## Installation

To use scripts in this repository, clone and run.

```bash
git clone git@github.com:AnqiW222/CMEECourseWork.git
```

## Contents
### Week 1: UNIX, Shell scription, LaTeX and Version Control with Git
**Summary:** A brief introduction of the Multilingual Quantitative Biological Methods, UNIX basic knowledge, Introductory shell scripting exercises, Produce scientific documents with LaTeX, and Use version control (Git) to share the files with others<br />
**Language Use:** Bash, LaTex

---

### Week 2: Basic Python Programming
**Summary:** Introduction to writing Python scripts/programs<br />
**Language Use:** Python, Bash

---

### Week 3: R Programming and Data Management & Visualizatio 
**Summary:** Biological Computing in R, Data management and Visualization with R.<br />
**Language Use:** R, LaTeX, Bash

---

### Week 4: Statistics in R
**Summary:** Core Skills Module of PG Life Science, statistical methods that are of wide use in research projects, the different ways of analysing data and the importance of biological interpretation. <br />
**Language Use:** R

---

### Week 5: Spatial Analyses and GIS
**Summary:** Core Skills Module of PG Life Science, using and handling GIS data, along with core concepts in GIS and remote sensing. <br />
**Language Use:** R

---

### Week 6: Genomics and Bioinformatics
**Summary:** Core Skills Module of PG Life Science, introduce the types of questions that can be addressed with population genomic data, and the theory and computational methodologies that are available for answering these questions. <br />
**Language Use:** R

---

### Week 7: Advanced Python Programming
**Summary:** Advanced Python coding skills with introduction of IDE, profiling code, and using computing language flexible. <br />
**Language Use:** Python, R, Bash

---

### Week 8 + 9: MiniProject
**Summary:** MSc CMEE Miniproject: i) What mathematical models best fit to an empirical dataset; ii) Based upon bacteria growth, mechanistic vs. phenomenological models, which is the best fit. Using all biological computing tools learned so far, from data pre-processing, model fitting, plotting and analysis results, to coding and academic report writing, solve the ecological modelling question.<br />
**Language Use:** Python, R, LaTeX, Bash

---

### Week 10 + 11: High Performance Computing and Math Primer
**Summary:** Using Imperial College's HPC cluster as tools and techniques  to solve biological problems, and dealing with the huge data sets through parallel computing. Introduction to the preliminary requirements for the topics that will be covered during the Maths for Biologists module. <br />
**Language Use:** R, Bash, HPC

## Language Versions
**Python:** 3.9.12 <br />
**R:** 4.2.1 <br />
**bash:** 3.2 <br />
**LaTeX:** 3.141592653-2.6-1.40.24 (TeX Live 2022) <br />
**Jupyter:** Notebook 6.4.8 <br />

All code has been written on a MacOS version 12.6 and any dependencies are detailed below the script names within weekly README files


**********************************************************************

======================================================================
Looking for the weekly directories...

Found 7 weekly directories: week1, week2, week3, week4, week5, week6, week7

The Week5 directory will be assessed 

======================================================================
======================================================================
PART 2: Checking weekly code and workflow...

======================================================================
Assessing WEEK5...

Found the following directories: code, results, data

Found the following files: README.md, .gitignore

Checking for readme file in weekly directory...

Found README in parent directory, named: README.md

Printing contents of README.md:

**********************************************************************
# PG Life Sciences Core Skills Modules - week 5:

This README file contains details about the scripts from in-classwork and practicals for the fifth week.

## Description
More information about the R scripts and programs for practicals in [GIS and Spatial Methoods in R](https://davidorme.github.io/Masters_GIS/intro.html)

## Language

R

## Dependencies
For some scripts in this directory, packages [terra](https://cran.r-project.org/web/packages/terra/index.html), [sf](https://cran.r-project.org/web/packages/sf/index.html), [raster](https://cran.r-project.org/web/packages/raster/index.html), [geodata](https://cran.r-project.org/web/packages/geodata/index.html), [sp](https://cran.r-project.org/web/packages/sp/index.html), [rgeos](https://cran.r-project.org/web/packages/rgeos/index.html), [rgdal](https://cran.r-project.org/web/packages/rgdal/index.html), [lwgeom](https://cran.r-project.org/web/packages/lwgeom/index.html), [openxlsx](https://cran.r-project.org/web/packages/openxlsx/index.html), [ggplot2](https://cran.r-project.org/web/packages/ggplot2/index.html), [gridExtra](https://cran.r-project.org/web/packages/gridExtra/index.html), [dismo](https://cran.r-project.org/web/packages/dismo/index.html), [ncf](https://cran.r-project.org/web/packages/ncf/index.html), [SpatialPack](https://cran.r-project.org/web/packages/SpatialPack/index.html), [spdep](https://cran.r-project.org/web/packages/spdep/index.html), [spatialreg](https://cran.r-project.org/web/packages/spatialreg/index.html), [nlme](https://cran.r-project.org/web/packages/nlme/index.html), [spgwr](https://cran.r-project.org/web/packages/spgwr/index.html),and [spmoran](https://cran.r-project.org/web/packages/spmoran/index.html) are required. 
Please run the following script in **R/RStudio** for package installation: 
```R
install.packages(c("terra", "sf", "raster", "geodata", "sp", "rgeos", "rgdal", "lwgeom", "openxlsx", "ggplot2", "gridExtra", "dismo", "ncf", "SpatialPack", "spdep", "spatialreg", "nlme", "spgwr", "spmoran"))
```

_The installation commands are used for MacOS, may varied with the different operating system._

## R

#### GIS_prepare.R:
<font size=2>**Summary:** All the required packages for each practicals.<br />
**Input:** n/a <br />
**Output:** package installation <br />
**Running Instructions:** Rscript GIS_prepare.R <br /><br /></font>

#### GISPractical1.R:
<font size=2>**Summary:** GIS data and plotting.<br />
**Input:** ../data/*.* <br />
**Output:** Results prints in terminals. <br />
**Running Instructions:** Rscript GISPractical1.R <br /><br /></font>

#### GISPractical2.R:
<font size=2>**Summary:** Species Distribution Modelling.<br />
**Input:** ../data/*.* <br />
**Output:** Results prints in terminals. <br />
**Running Instructions:** Rscript GISPractical2.R <br /><br /></font>

#### GISPractical3.R:
<font size=2>**Summary:** Spatial modelling in R.<br />
**Input:** ../data/*.* <br />
**Output:** Results prints in terminals. <br />
**Running Instructions:** Rscript GISPractical3.R <br /><br /></font>


## Author & Contact

<font size=2>**Name:** ANQI WANG<br />
**Email:** aw222@ic.ac.uk</font>
**********************************************************************

Results directory is empty - good! 

Found 4 code files: GIS_prepare.R, GISPractical2.R, GISPractical1.R, GISPractical3.R

======================================================================
Testing script/code files...

======================================================================
Inspecting script file GIS_prepare.R...

File contents are:

**********************************************************************
# Core GIS function and data packages
# Install the following packages to start
install.packages('terra')    # Core raster GIS data package
install.packages('sf')       # Core vector GIS data package
install.packages('raster')   # Older raster GIS package required by some packages
install.packages('geodata')  # Data downloader

install.packages('sp')        # Older vector GIS package - replaced by sf in most cases
install.packages('rgeos')     # Extended vector data functionality
install.packages('rgdal')     # Interface to the Geospatial Data Abstraction Library
install.packages('lwgeom')    # Lightweight geometry engine

# Practical 1
install.packages('openxlsx')   # Read data from Excel 
install.packages('ggplot2')    # Plotting package
install.packages('gridExtra')  # Extensions to ggplot

# Practical 2
install.packages('dismo')      # Species distribution models

#Practical 3
install.packages('ncf')          
install.packages('SpatialPack')  # Spatial clifford test
install.packages('spdep')        # Spatial dependence models
install.packages('spatialreg')   # Spatial regression using lags
install.packages('nlme')         # GLS models
install.packages('spgwr')        # Geographic weighted regression
install.packages('spmoran')      # Moran's I and Geary C

# Landscape Ecology Week
install.packages('landscapemetrics')
install.packages('vegan')

**********************************************************************

Testing GIS_prepare.R...

Output (only first 500 characters): 


**********************************************************************

**********************************************************************

Encountered error (or warning):

***IGNORE IF THIS ERROR IS EXPECTED AS PART OF AN IN-CLASS EXERCISE***

Installing package into ‘/usr/local/lib/R/site-library’
(as ‘lib’ is unspecified)
Warning in install.packages("terra") :
  'lib = "/usr/local/lib/R/site-library"' is not writable
Error in install.packages("terra") : unable to install packages
Execution halted

======================================================================
Inspecting script file GISPractical2.R...

File contents are:

**********************************************************************
################################################################################
##################                                             #################
################# Practical Two: Species Distribution Modelling ################
##################                                             #################
################################################################################
## Author: Anqi Wang 
##
## Date Created: Oct, 2022
##
## Email: aw222@ic.ac.uk

# Required packages
#install.packages('dismo')      # Species distribution models
#raster_obj <- as(spat_raster_obj, 'Raster')

# Load the packages
library(terra)
library(geodata)

library(raster)
library(sf)
library(sp)

library(dismo)

######################
## Data preparation ##
######################

## Focal species distribution ##
# be critical of point observation data and carefully clean it
vignette('sdm')

# view both kinds of data for this species
# The IUCN data is a single MULTIPOLYGON feature showing the discontinuous sections of the species’ range. 
# There are a number of feature attributes, described in detail in this pdf.
tapir_IUCN <- st_read('../data/sdm/iucn_mountain_tapir/data_0.shp')
print(tapir_IUCN)
# The GBIF data is a table of observations, some of which include coordinates. 
# One thing that GBIF does is to publish a DOI for every download, to make it easy to track particular data use. 
# This one is https://doi.org/10.15468/dl.t2bkzx.

# tricks to loading the GBIF data:
# Load the data frame
tapir_GBIF <- read.delim('../data/sdm/gbif_mountain_tapir.csv')

# Drop rows with missing coordinates
tapir_GBIF <- subset(tapir_GBIF, ! is.na(decimalLatitude) | ! is.na(decimalLongitude))

# Convert to an sf object and set the projection
tapir_GBIF <- st_as_sf(tapir_GBIF, coords=c('decimalLongitude', 'decimalLatitude'))
st_crs(tapir_GBIF) <- 4326
# superimpose the two datasets to show they broadly agree. 
# There aren’t any obvious problems that require data cleaning.
# Load some (coarse) country background data
ne110 <- st_read('../data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp')

# Create a modelling extent for plotting and cropping the global data.
model_extent <- extent(c(-85,-70,-5,12))

# Plot the species data over a basemap
plot(st_geometry(ne110), xlim=model_extent[1:2], ylim=model_extent[3:4], 
     bg='lightblue', col='ivory', axes=TRUE)
plot(st_geometry(tapir_IUCN), add=TRUE, col='grey', border=NA)
plot(st_geometry(tapir_GBIF), add=TRUE, col='red', pch=4, cex=0.6)

## Predictor variables ##
# Both these datasets are sourced from http://www.worldclim.org 
# and contain a stack of the 19 BIOCLIM variables at 10 arc-minute resolution (10′=1/6°≈15km).
# Load the data
bioclim_hist <- worldclim_global(var='bio', res=10, path='data')
bioclim_future <- cmip6_world(var='bioc', res=10, ssp="585", 
                              model='HadGEM3-GC31-LL', time="2041-2060", path='data')

# Relabel the variables to match between the two dataset
bioclim_names <- paste0('bio', 1:19)
names(bioclim_future) <- bioclim_names
names(bioclim_hist) <- bioclim_names

# Look at the data structure
print(bioclim_hist)
print(bioclim_future)

# compare BIO 1 (Mean Annual Temperature) between the two datasets:
par(mfrow=c(2,2), mar=c(1,1,1,1))

# Create a shared colour scheme
breaks <- seq(-30, 35, by=2)
cols <- hcl.colors(length(breaks) - 1)

# Plot the historical and projected data
plot(bioclim_hist[[1]], breaks=breaks, col=cols, 
     type='continuous', plg=list(ext=c(190,200,-90,90)))
plot(bioclim_future[[1]], breaks=breaks, col=cols, 
     type='continuous', plg=list(ext=c(190,200,-90,90)))

# Plot the temperature difference
plot(bioclim_future[[1]] - bioclim_hist[[1]], 
     col=hcl.colors(20, palette='Inferno'), breakby='cases',
     type='continuous', plg=list(ext=c(190,200,-90,90)))

# crop the environmental data down to a sensible modelling region. 
# Reduce the global maps down to the species' range
bioclim_hist_local <- crop(bioclim_hist, model_extent)
bioclim_future_local <- crop(bioclim_future, model_extent)

## Reproject the data ##
# For the raster data, if just specify the projection, then the terra package picks a resolution and extent that best approximates the original data.
test <-  project(bioclim_hist_local, 'EPSG:32718')
ext(test)

res(test)  # Resolution in metres on X and Y axes

# create the specific grid we want and project the data into that.
# Define a new projected grid
utm18s_grid <- rast(ext(-720000, 1180000, 9340000, 11460000), 
                    res=20000, crs='EPSG:32718')

# Reproject the model data
bioclim_hist_local <- project(bioclim_hist_local, utm18s_grid)
bioclim_future_local <- project(bioclim_future_local, utm18s_grid)
# reproject the species distribution vector data:
tapir_IUCN <- st_transform(tapir_IUCN, crs='EPSG:32718')
tapir_GBIF <- st_transform(tapir_GBIF, crs='EPSG:32718')

## Pseudo-absence data ##
# Many of the methods below require absence data, either for fitting a model or for evaluating the model performance.
# Rarely, we might actually have real absence data from exhaustive surveys, but usually we only have presence data. 
# So, modelling commonly uses pseudo-absence or background locations. 
# The difference between those two terms is subtle: I haven’t seen a clean definition but background data might be sampled completely at random, 
# where pseudo-absence makes some attempt to pick locations that are somehow separated from presence observations.

# The dismo package provides the randomPoints function to select background data.
# works directly with the environmental layers to pick points representing cells.
# avoids getting duplicate points in the same cells
# provide a mask layer that shows which cells are valid choices:
# Create a simple land mask
land <- bioclim_hist_local[[1]] >= 0

# How many points to create? We'll use the same as number of observations
n_pseudo <- nrow(tapir_GBIF)

# Sample the points
pseudo_dismo <- randomPoints(mask=as(land, 'Raster'), n=n_pseudo, 
                             p=st_coordinates(tapir_GBIF))

# Convert this data into an sf object, for consistency with the
# next example.
pseudo_dismo <- st_as_sf(data.frame(pseudo_dismo), coords=c('x','y'), crs=32718)
# use GIS to pick points that are within 100 km of observed points, but not closer than 20km:
# Create buffers around the observed points
nearby <- st_buffer(tapir_GBIF, dist=100000)
too_close <- st_buffer(tapir_GBIF, dist=20000)
# merge those buffers
nearby <- st_union(nearby)
too_close <- st_union(too_close)
# Find the area that is nearby but _not_ too close
nearby <- st_difference(nearby, too_close)
# Get some points within that feature in an sf dataframe
pseudo_nearby <- st_as_sf(st_sample(nearby, n_pseudo))

# plot those two points side by side for comparison:
par(mfrow=c(1,2), mar=c(1,1,1,1))

# Random points on land
plot(land, col='grey', legend=FALSE)
plot(st_geometry(tapir_GBIF), add=TRUE, col='red')
plot(pseudo_dismo, add=TRUE)

# Random points within ~ 100 km but not closer than ~ 20 km
plot(land, col='grey', legend=FALSE)
plot(st_geometry(tapir_GBIF), add=TRUE, col='red')
plot(pseudo_nearby, add=TRUE)

## Testing and training dataset ##
# One important part of the modelling process is to keep separate data 
# for training the model (the process of fitting the model to data) and for testing the model (the process of checking model performance). 
# a 20:80 split - retaining 20% of the data for testing:
# Use kfold to add labels to the data, splitting it into 5 parts
tapir_GBIF$kfold <- kfold(tapir_GBIF, k=5)

# Do the same for the pseudo-random points
pseudo_dismo$kfold <- kfold(pseudo_dismo, k=5)
pseudo_nearby$kfold <- kfold(pseudo_nearby, k=5)

# One other important concept in test and training is cross validation. 
# This is where a model is fitted and tested multiple times, using different subsets of the data, 
# to check that model performance is not dependent on one specific partition of the data. 
# One common approach is k-fold cross-validation (hence the function name above). 
# This splits the data into k partitions and then uses each partition in turn as the test data.


####################################
## Species Distribution Modelling ##
####################################

## The BIOCLIM model ##
# It is not a particularly good method, but it is possible to fit the model and predict with only species presence data and without using (pseudo-)absence data. 
#The way the model works is to sample environmental layers at species locations. 
# A cell in the wider map then gets a score based on how close to the species’ median value for each layer it is.
# Name comes from bioclimate envelope

# Fitting the model #
#  need the environmental layers and a matrix of XY coordinates for the training data showing where the species is observed to fit a bioclimate envelope
# Get the coordinates of 80% of the data for training 
train_locs <- st_coordinates(subset(tapir_GBIF, kfold != 1))

# Fit the model
bioclim_model <- bioclim(as(bioclim_hist_local, 'Raster'), train_locs)
#The st_coordinates function is a useful sf function for extracting point coordinates: it does also work with lines and polygons, but the output is much more complex!

#  plot the model output to show the envelopes. 
par(mfrow=c(1,2))
plot(bioclim_model, a=1, b=2, p=0.9)
plot(bioclim_model, a=1, b=5, p=0.9)
# Note:
# the argument p is used to show the climatic envelop that contains a certain proportion of the data. 
# The a and b arguments set which layers in the environmental data are compared.

# Model predictions #
# use the model parameters to predict the bioclim score for the whole map.
# Note that a lot of the map has a score of zero: none of the environmental variables in these cells fall within the range seen in the occupied cells.
bioclim_pred <- predict(bioclim_hist_local, bioclim_model)

# Create a copy removing zero scores to focus on within envelope locations
bioclim_non_zero <- bioclim_pred
bioclim_non_zero[bioclim_non_zero == 0] <- NA
plot(land, col='grey', legend=FALSE)
plot(bioclim_non_zero, col=hcl.colors(20, palette='Blue-Red'), add=TRUE)

# Model evaluation #
# Note: the absence data has to be provided: 
# all of the standard performance metrics come from a confusion matrix, which requires false and true negatives. 
# The output of evaluate gives us some key statistics, including AUC.
test_locs <- st_coordinates(subset(tapir_GBIF, kfold == 1))
test_pseudo <- st_coordinates(subset(pseudo_nearby, kfold == 1))
bioclim_eval <- evaluate(p=test_locs, a=test_pseudo, 
                         model=bioclim_model, x=bioclim_hist_local)
print(bioclim_eval)

# create some standard plots: 
# the ROC curve
# a graph of how kappa changes as the threshold used on the model predictions varies. 
# The threshold function allows to find an optimal threshold for a particular measure of performance:
# In this case: find the threshold that gives the best kappa.
par(mfrow=c(1,2))

# Plot the ROC curve
plot(bioclim_eval, 'ROC', type='l')

# Find the maximum kappa and show how kappa changes with the model threshold
max_kappa <- threshold(bioclim_eval, stat='kappa')
plot(bioclim_eval, 'kappa', type='l')
abline(v=max_kappa, lty=2, col='blue')

# Species distribution #
# contains all the information we need to make a prediction about the species distribution:
# Apply the threshold to the model predictions
par(mfrow=c(1,1))
tapir_range <- bioclim_pred >= max_kappa
plot(tapir_range, legend=FALSE, col=c('khaki3','red'))
plot(st_geometry(tapir_GBIF), add=TRUE, pch=4, col='#00000088')

# Future distribution of the tapir #
# Predict from the same model but using the future data
bioclim_pred_future <- predict(bioclim_future_local, bioclim_model)
# Apply the same threshold
tapir_range_future <- bioclim_pred_future >= max_kappa

par(mfrow=c(1,3), mar=c(2,2,1,1))
plot(tapir_range, legend=FALSE, col=c('grey','red'))
plot(tapir_range_future, legend=FALSE, col=c('grey','red'))

# This is a bit of a hack - adding 2 * hist + 2050 gives:
# 0 + 0 - present in neither model
# 0 + 1 - only in future
# 2 + 0 - only in hist
# 2 + 1 - in both
tapir_change <- 2 * (tapir_range) + tapir_range_future
cols <- c('lightgrey', 'blue', 'red', 'forestgreen')
plot(tapir_change, col=cols, legend=FALSE)
legend('topleft', fill=cols, legend=c('Absent','Future','Historical', 'Both'), bg='white')

## Generalised Linear Model (GLM) ##
# a kind of regression that allows us to model presence/absence as a binary response variable more appropriately. 

# Data restructuring #
# The bioclim model allowed to provide points and some maps but many other distribution models require to use a model formula, 
# need to restructure our data into a data frame of species presence/absence and the environmental values observed at the locations of those observations.

# First, we need to combine tapir_GBIF and pseudo_nearby into a single data frame:
# Create a single sf object holding presence and pseudo-absence data.
# - reduce the GBIF data and pseudo-absence to just kfold and a presence-absence value
present <- subset(tapir_GBIF, select='kfold')
present$pa <- 1
absent <- pseudo_nearby
absent$pa <- 0

# - rename the geometry column of absent to match so we can stack them together.
names(absent) <- c('geometry','kfold','pa')
st_geometry(absent) <- 'geometry'

# - stack the two dataframes
pa_data <- rbind(present, absent)
print(pa_data)

# Second, we need to extract the environmental values for each of those points and add it into the data frame.
envt_data <- extract(bioclim_hist_local, pa_data)
pa_data <- cbind(pa_data, envt_data)
print(pa_data)

# Fitting the GLM #
glm_model <- glm(pa ~ bio2 + bio4 + bio3 + bio1 + bio12, data=pa_data, 
                 family=binomial(link = "logit"),
                 subset=kfold != 1)
# Using a GLM gives us the significance of different variables - this table is very similar to a linear model summary
# Look at the variable significances - which are important
summary(glm_model)

# to look at response plots: these show how the probability of a species being present changes with a given variable. 
# These are predictions for each variable in turn, holding all the other variables at their median value
# Response plots
par(mar=c(3,3,1,1), mgp=c(2,1,0))
response(glm_model, fun=function(x, y, ...) predict(x, y, type='response', ...))

# Model predictions and evaluation #
# Create a prediction layer:
glm_pred <- predict(bioclim_hist_local, glm_model, type='response')

# Evaluate the model using the test data
# Extract the test presence and absence
test_present <- st_coordinates(subset(pa_data, pa == 1 & kfold == 1))
test_absent <- st_coordinates(subset(pa_data, pa == 0 & kfold == 1))
glm_eval <- evaluate(p=test_present, a=test_absent, model=glm_model, 
                     x=bioclim_hist_local)
print(glm_eval)

# Find the maximum kappa threshold. This is a little more complicated than before the threshold we get is again on the scale of the linear predictor. 
max_kappa <- plogis(threshold(glm_eval, stat='kappa'))
print(max_kappa)

# Look at some model performance plots:
par(mfrow=c(1,2))
# ROC curve and kappa by model threshold
plot(glm_eval, 'ROC', type='l')
plot(glm_eval, 'kappa', type='l')
abline(v=max_kappa, lty=2, col='blue')

# Species distribution from GLM #
# use the threshold to convert the model outputs into a predicted species’ distribution map and compare current suitability to future suitability
par(mfrow=c(2,2))

# Modelled probability
plot(glm_pred, col=hcl.colors(20, 'Blue-Red'))

# Threshold map
glm_map <- glm_pred >= max_kappa
plot(glm_map, legend=FALSE, col=c('grey','red'))

# Future predictions
glm_pred_future <- predict(bioclim_future_local, glm_model, type='response')
plot(glm_pred_future, col=hcl.colors(20, 'Blue-Red'))

# Threshold map
glm_map_future <- glm_pred_future >= max_kappa
plot(glm_map_future, legend=FALSE, col=c('grey','red'))

# simple way to describe the modelled changes is simply to look at a table of the pair of model predictions:
table(values(glm_map), values(glm_map_future), dnn=c('hist', '2050'))


########################################
## Sandbox - things to try with SDMs ##
########################################

## Modelling a different species ##
# Check the species without downloading - this shows the number of records
gbif('Tapirus', 'pinchaque', download=FALSE)
# Download the data
locs <- gbif('Tapirus', 'pinchaque')
locs <- subset(locs, ! is.na(lat) | ! is.na(lon))
# Convert to an sf object 
locs <- st_as_sf(locs, coords=c('lon', 'lat'))

## Does our background choice matter? ##
# 1. Create the new dataset
present <- subset(tapir_GBIF, select='kfold')
present$pa <- 1
absent <- pseudo_dismo
absent$pa <- 0

# - rename the geometry column of absent to match so we can stack them together.
names(absent) <- c('geometry','kfold','pa')
st_geometry(absent) <- 'geometry'
# - stack the two dataframes
pa_data_bg2 <- rbind(present, absent)
# - Add the envt.
envt_data <- extract(bioclim_hist_local, pa_data_bg2)
pa_data_bg2 <- cbind(pa_data_bg2, envt_data)

# 2. Fit the model
glm_model_bg2 <-glm(pa ~ bio2 + bio4 + bio3 + bio1 + bio12, data=pa_data_bg2, 
                    family=binomial(link = "logit"),
                    subset=kfold != 1)

# 3. New predictions
glm_pred_bg2 <- predict(bioclim_hist_local, glm_model_bg2, type='response')

# 4. Plot modelled probability using the same colour scheme and using
# axis args to keep a nice simple axis on the legend
par(mfrow=c(1,3))
bks <- seq(0, 1, by=0.01)
cols <- hcl.colors(100, 'Blue-Red')
plot(glm_pred, col=cols, breaks=bks, main='Buffered Background', 
     type='continuous', plg=list(ext=c(1.25e6, 1.3e6, 9.3e6, 11.5e6)))
plot(glm_pred_bg2, col=cols, breaks=bks, main='Extent Background',
     type='continuous', plg=list(ext=c(1.25e6, 1.3e6, 9.3e6, 11.5e6)))
plot(glm_pred - glm_pred_bg2, col= hcl.colors(100), main='Difference',
     type='continuous', plg=list(ext=c(1.25e6, 1.3e6, 9.3e6, 11.5e6)))

## Cross validation ##
# A better approach is to perform k-fold cross validation to check that the model behavour is consistent across different partitions. 
# which allows to compare the performance of models taking into account the arbitrary structure of the partitioning.

# A way to get the ROC data from the output of evaluate - this is not obvious because dismo uses a different coding approach (S4 methods) that is a bit more obscure. 
get_roc_data <- function(eval){
  #' get_roc_data
  #' 
  #' This is a function to return a dataframe of true positive
  #' rate and false positive rate from a dismo evalulate output
  #' 
  #' @param eval An object create by `evaluate` (S4 class ModelEvaluation)
  
  roc <- data.frame(FPR = eval@FPR, TPR = eval@TPR)
  return(roc)
}

# each model evaluation to be carried out using the same threshold breakpoints
# Take a sequence of probability values from 0 to 1
thresholds <- seq(0, 1, by=0.01)
# Convert to the default scale for a binomial GLM
thresholds <- qlogis(thresholds)
# Use in evaluate
eval <- evaluate(p=test_present, a=test_absent, model=glm_model, 
                 x=bioclim_hist_local, tr=thresholds)

# Make some objects to store the data
tpr_all <- matrix(ncol=5, nrow=length(thresholds))
fpr_all <- matrix(ncol=5, nrow=length(thresholds))
auc <- numeric(length=5)

# Loop over the values 1 to 5
for (test_partition in 1:5) {
  
  # Fit the model, holding back this test partition
  model <- glm(pa ~ bio2 + bio4 + bio3 + bio1 + bio12, data=pa_data, 
               family=binomial(link = "logit"),
               subset=kfold != test_partition)
  
  # Evaluate the model using the retained partition
  test_present <- st_coordinates(subset(pa_data, pa == 1 & kfold == test_partition))
  test_absent <- st_coordinates(subset(pa_data, pa == 0 & kfold == test_partition))
  eval <- evaluate(p=test_present, a=test_absent, model=glm_model, 
                   x=bioclim_hist_local, tr=thresholds)
  
  # Store the data
  auc[test_partition] <- eval@auc
  roc <- get_roc_data(eval)
  tpr_all[,test_partition] <- roc$TPR
  fpr_all[,test_partition] <- roc$FPR
}

# Create a blank plot to showing the mean ROC and the individual traces
plot(rowMeans(fpr_all), rowMeans(tpr_all), type='n')

# Add the individual lines
for (test_partition in 1:5) {
  lines(fpr_all[, test_partition], tpr_all[, test_partition], col='grey')
}

# Add the mean line
lines(rowMeans(fpr_all), rowMeans(tpr_all))

print(auc)
print(mean(auc))

###################################
## Reducing the set of variables ##
###################################
# We can use the values from the environmental data in a cluster algorithm.
# This is a statistical process that groups sets of values by their similarity
# and here we are using the local raster data (to get a good local sample of the 
# data correlation) to perform that clustering.
clust_data <- values(bioclim_hist_local)
clust_data <- na.omit(clust_data)

# Scale and center the data
clust_data <- scale(clust_data)

# Transpose the data matrix to give variables as rows
clust_data <- t(clust_data)

# Find the distance between variables and cluster
clust_dist <- dist(clust_data)
clust_output <- hclust(clust_dist)
plot(clust_output)

# And then pick one from each of five blocks - haphazardly.
rect.hclust(clust_output, k=5)

**********************************************************************

Testing GISPractical2.R...

Output (only first 500 characters): 


**********************************************************************

**********************************************************************

Encountered error (or warning):

***IGNORE IF THIS ERROR IS EXPECTED AS PART OF AN IN-CLASS EXERCISE***

Error: package or namespace load failed for ‘terra’ in .doLoadActions(where, attach):
 error in load action .__A__.1 for package terra: loadModule(module = "spat", what = TRUE, env = ns, loadNow = TRUE): Unable to load module "spat": function 'Rcpp_precious_remove' not provided by package 'Rcpp'
Execution halted

======================================================================
Inspecting script file GISPractical1.R...

File contents are:

**********************************************************************
################################################################################
#######################                                    #####################
###################### Practical One: GIS data and plotting ####################
#######################                                    #####################
################################################################################
## Author: Anqi Wang 
##
## Date Created: Oct, 2022
##
## Email: aw222@ic.ac.uk

# Load the packages
library(terra)     # core raster GIS package
library(sf)        # core vector GIS package
library(units)     # used for precise unit conversion

library(geodata)   # Download and load functions for core datasets
library(openxlsx)  # Reading data from Excel files

# Turn off an advanced feature used by the sf package
sf_use_s2(FALSE)

# Always have a clean workplace
rm(list = ls())
#################
## Vector Data ##
#################

## Making vectors from coordinates ##
# Create a population density map for the British lsles #
pop_dens <- data.frame(
  n_km2 = c(260, 67,151, 4500, 133), 
  country = c('England','Scotland', 'Wales', 'London', 'Northern Ireland')
)
print(pop_dens)

# simple polygons to show the countries #
# Create coordinates  for each country 
# - this creates a matrix of pairs of coordinates forming the edge of the polygon. 
# - note that they have to _close_: the first and last coordinate must be the same.
scotland <- rbind(c(-5, 58.6), c(-3, 58.6), c(-4, 57.6), 
                  c(-1.5, 57.6), c(-2, 55.8), c(-3, 55), 
                  c(-5, 55), c(-6, 56), c(-5, 58.6))
england <- rbind(c(-2,55.8),c(0.5, 52.8), c(1.6, 52.8), 
                 c(0.7, 50.7), c(-5.7,50), c(-2.7, 51.5), 
                 c(-3, 53.4),c(-3, 55), c(-2,55.8))
wales <- rbind(c(-2.5, 51.3), c(-5.3,51.8), c(-4.5, 53.4),
               c(-2.8, 53.4),  c(-2.5, 51.3))
ireland <- rbind(c(-10,51.5), c(-10, 54.2), c(-7.5, 55.3),
                 c(-5.9, 55.3), c(-5.9, 52.2), c(-10,51.5))

# Convert these coordinates into feature geometries #
# - these are simple coordinate sets with no projection information
scotland <- st_polygon(list(scotland))
england <- st_polygon(list(england))
wales <- st_polygon(list(wales))
ireland <- st_polygon(list(ireland))

# Combine these into a simple feature column (sfc). 
# Use the list of geometries to include vector data in a normal R data.frame, 
# also used to set the coordinate reference system (crs or projection) of the data.

# Combine geometries into a simple feature column
uk_eire_sfc <- st_sfc(wales, england, scotland, ireland, crs=4326)
plot(uk_eire_sfc, asp=1) 
# sf automatically tries to scale the aspect ratio of plots of geographic coordinate data 
# (coordinates are latitude and longitude) based on their latitude - this makes them look less squashed. 
# We are actively suppressing that here by setting an aspect ratio of one (asp=1).

## Making vector points from a dataframe ##

# Creates point locations for capital cities #
uk_eire_capitals <- data.frame(
  long= c(-0.1, -3.2, -3.2, -6.0, -6.25),
  lat=c(51.5, 51.5, 55.8, 54.6, 53.30),
  name=c('London', 'Cardiff', 'Edinburgh', 'Belfast', 'Dublin')
)

# Indicate which fields in the data frame contain the coordinates
uk_eire_capitals <- st_as_sf(uk_eire_capitals, coords=c('long','lat'), crs=4326)
print(uk_eire_capitals)

# Vector Geometry operations #
# Current issues: 1. missing separate polygon for London
# 2. The boundary for Wales is poorly digitized - we want a common border with England.
# 3. Northern Ireland has not separated from Eire. #

# use the buffer operation to create a polygon for London, which we define as anywhere within a quarter degree of St. Pauls Cathedral. 
st_pauls <- st_point(x=c(-0.098056, 51.513611))
london <- st_buffer(st_pauls, 0.25)

# use the difference operation to remove London from England polygon
england_no_london <- st_difference(england, london) # the order of the argument matters!!!

# lengths function allows to see the number of components in polygon and how many in each
lengths(scotland)
# a single component with 18 points
lengths(england_no_london)

# use the same operation to tidy up Wales: in this case we want the bits of Wales that are different from England.
wales <- st_difference(wales, england)
# use the intersection operation to separate Northern Ireland from the island of Ireland.
# A rough polygon that includes Northern Ireland and surrounding sea.
# - not the alternative way of providing the coordinates
ni_area <- st_polygon(list(cbind(x=c(-8.1, -6, -5, -6, -8.1), y=c(54.4, 56, 55, 54, 54.4))))

northern_ireland <- st_intersection(ireland, ni_area)
eire <- st_difference(ireland, ni_area)

# Combine the final geometries
uk_eire_sfc <- st_sfc(wales, england_no_london, scotland, london, northern_ireland, eire, crs=4326)
# plot(uk_eire_sfc)

## Features and geometries ##
# create a single feature that contains all of those geometries in one MULTIPOLYGON geometry by using the union operation
# compare six Polygon features with one Multipolygon feature
print(uk_eire_sfc)
# make the UK into a single feature
uk_country <- st_union(uk_eire_sfc[-6])
print(uk_country)
# Plot them
par(mfrow=c(1, 2), mar=c(3,3,1,1))
plot(uk_eire_sfc, asp=1, col=rainbow(6))
plot(st_geometry(uk_eire_capitals), add=TRUE)
plot(uk_country, asp=1, col='lightblue')

## Vector data and attribute ##
# The sf package does this using the sf object type: 
# basically this is just a normal data frame with that additional field containing simple feature data. 
# We can do that here - printing the object shows some extra information compared to a basic data.frame.
uk_eire_sf <- st_sf(name=c('Wales', 'England','Scotland', 'London', 
                           'Northern Ireland', 'Eire'),
                    geometry=uk_eire_sfc)

print(uk_eire_sf)
# An sf object also has a simple plot method, which we can use to draw a basic map
plot(uk_eire_sf['name'], asp=1)
# Since an sf object is an extended data frame, we can add attributes by adding fields directly:
uk_eire_sf$capital <- c('Cardiff', 'London', 'Edinburgh', 
                        NA, 'Belfast','Dublin')
print(uk_eire_sf)
# The merge function allows us to set columns in two data frames that containing matching values and uses those to merge the data together.
uk_eire_sf <- merge(uk_eire_sf, pop_dens, by.x='name', by.y='country', all.x=TRUE)
print(uk_eire_sf)

## Spatial attribute ##
#  the centroids of features.
uk_eire_centroids <- st_centroid(uk_eire_sf)
st_coordinates(uk_eire_centroids)

# Two other simple ones are the length of a feature and its area. 
# A given goegraphic coordinate system and instead uses internal transformations 
# to give us back accurate distances and areas using metres. 
# Under the hood, it is using calculations on the surface of a sphere, so called great circle distances.
uk_eire_sf$area <- st_area(uk_eire_sf)

# To calculate a 'length' of a polygon, you have to convert it to a LINESTRING or a 
# MULTILINESTRING. Using MULTILINESTRING will automatically include all perimeter of a 
# polygon (including holes).
uk_eire_sf$length <- st_length(st_cast(uk_eire_sf, 'MULTILINESTRING'))

# Look at the result
print(uk_eire_sf)

# You can change units in a neat way
uk_eire_sf$area <- set_units(uk_eire_sf$area, 'km^2')
uk_eire_sf$length <- set_units(uk_eire_sf$length, 'km')
print(uk_eire_sf)

# And it won't let you make silly error like turning a length into weight
uk_eire_sf$area <- set_units(uk_eire_sf$area, 'kg')

# Or you can simply convert the `units` version to simple numbers
uk_eire_sf$length <- as.numeric(uk_eire_sf$length)
#print(uk_eire_sf)

# A final useful example is the distance between objects: 
# sf gives us the closest distance between geometries, 
# which might be zero if two features overlap or touch, as in the neighbouring polygons in our data
st_distance(uk_eire_sf)
# sf is noting that we have a geographic coordinate system and internally calculating distances in metres.
st_distance(uk_eire_centroids)

## Plottng sf objects ##
# pick a single field to plot by using square brackets
plot(uk_eire_sf['n_km2'], asp=1)
# You could simply log the data:
uk_eire_sf$log_n_km2 <- log10(uk_eire_sf$n_km2)
plot(uk_eire_sf['log_n_km2'], asp=1, key.pos=4)
# Or you can have logarithimic labelling using logz
plot(uk_eire_sf['n_km2'], asp=1, logz=TRUE, key.pos=4)

## Reprojecting vector data ##
# reproject our UK and Eire map onto a good choice of local projected coordinate system: 
# British National Grid (EPSG:27700)
uk_eire_BNG <- st_transform(uk_eire_sf, 27700)
# UTM50N (EPSG:32650)
uk_eire_UTM50N <- st_transform(uk_eire_sf, 32650)
# The bounding boxes of the data shows the change in units
st_bbox(uk_eire_sf)
st_bbox(uk_eire_BNG)
# using the st_geometry function to only plot the geometry data and not a particular attribute
# Plot the results
par(mfrow=c(1, 3), mar=c(3,3,1,1))
plot(st_geometry(uk_eire_sf), asp=1, axes=TRUE, main='WGS 84')
plot(st_geometry(uk_eire_BNG), axes=TRUE, main='OSGB 1936 / BNG')
plot(st_geometry(uk_eire_UTM50N), axes=TRUE, main='UTM 50N')

# Proj4 strings #

# Degrees are not constant #
# Set up some points separated by 1 degree latitude and longitude from St. Pauls
st_pauls <- st_sfc(st_pauls, crs=4326)
one_deg_west_pt <- st_sfc(st_pauls - c(1, 0), crs=4326) # near Goring
one_deg_north_pt <-  st_sfc(st_pauls + c(0, 1), crs=4326) # near Peterborough
# Calculate the distance between St Pauls and each point
st_distance(st_pauls, one_deg_west_pt)
st_distance(st_pauls, one_deg_north_pt)
st_distance(st_transform(st_pauls, 27700), 
            st_transform(one_deg_west_pt, 27700))

# transform St Pauls to BNG and buffer using 25 km
london_bng <- st_buffer(st_transform(st_pauls, 27700), 25000)
# In one line, transform england to BNG and cut out London
england_not_london_bng <- st_difference(st_transform(st_sfc(england, crs=4326), 27700), london_bng)
# project the other features and combine everything together
others_bng <- st_transform(st_sfc(eire, northern_ireland, scotland, wales, crs=4326), 27700)
corrected <- c(others_bng, london_bng, england_not_london_bng)
# Plot that and marvel at the nice circular feature around London
par(mar=c(3,3,1,1))
plot(corrected, main='25km radius London', axes=TRUE)

############
## Raster ##
############
# par(mfrow = c(1,1)) # set the plot format, only print one plot in a page
## Creating a raster ##
# Create an empty raster object covering UK and Eire
uk_raster_WGS84 <- rast(xmin=-11,  xmax=2,  ymin=49.5, ymax=59, 
                        res=0.5, crs="EPSG:4326")
hasValues(uk_raster_WGS84)
# Add data to the raster - just use the cell numbers
values(uk_raster_WGS84) <- cells(uk_raster_WGS84)
print(uk_raster_WGS84)
# create a basic map of that, with the country borders over the top: 
# add=TRUE adds the vector data to the existing map and the other options set border and fill colours. 
# The ugly looking #FFFFFF44 is a RGBA colour code that gives us a transparent gray fill for the polygon.
plot(uk_raster_WGS84)
plot(st_geometry(uk_eire_sf), add=TRUE, border='black', lwd=2, col='#FFFFFF44')

## Changing raster resolution ##
# Define a simple 4 x 4 square raster
m <- matrix(c(1, 1, 3, 3,
              1, 2, 4, 3,
              5, 5, 7, 8,
              6, 6, 7, 7), ncol=4, byrow=TRUE)
square <- rast(m)

plot(square, legend=NULL)
text(square, digits=2)

## Aggregating rasters ##
# With aggregating, we choose an aggregation factor - how many cells to group - and then lump sets of cells together.
# Average values
square_agg_mean <- aggregate(square, fact=2, fun=mean)
plot(square_agg_mean, legend=NULL)
text(square_agg_mean, digits=2)
# Maximum values
square_agg_max <- aggregate(square, fact=2, fun=max)
plot(square_agg_max, legend=NULL)
text(square_agg_max, digits=2)
# Modal values for categories
square_agg_modal <- aggregate(square, fact=2, fun='modal')
plot(square_agg_modal, legend=NULL)
text(square_agg_modal, digits=2)

## Disaggregating rasters ##
# The disaggregate function also requires a factor, but this time the factor is the square root of the number of cells 
# to create from each cell, rather than the number to merge. 
# There is again a choice to make on what values to put in the cell.
# Simply duplicate the nearest parent value
square_disagg <- disagg(square, fact=2, method='near')
plot(square_disagg, legend=NULL)
text(square_disagg, digits=2)
# Another option is to interpolate between the values to provide a smoother gradient between cells.
# This does not make sense for a categorical variable.
# Use bilinear interpolation
square_interp <- disagg(square, fact=2, method='bilinear')
plot(square_interp, legend=NULL)
text(square_interp, digits=1)

## Resampling ##
## Reprojecting a raster ##
# It can’t be displayed using actual raster data because they always need to plot on a regular grid.
# However,  create vector grids, and using the new function st_make_grid and other vector tools, to represent the cell edges in the two raster grids so overplot them.
# make two simple `sfc` objects containing points in  the
# lower left and top right of the two grids
uk_pts_WGS84 <- st_sfc(st_point(c(-11, 49.5)), st_point(c(2, 59)), crs=4326)
uk_pts_BNG <- st_sfc(st_point(c(-2e5, 0)), st_point(c(7e5, 1e6)), crs=27700)

#  Use st_make_grid to quickly create a polygon grid with the right cellsize
uk_grid_WGS84 <- st_make_grid(uk_pts_WGS84, cellsize=0.5)
uk_grid_BNG <- st_make_grid(uk_pts_BNG, cellsize=1e5)

# Reproject BNG grid into WGS84
uk_grid_BNG_as_WGS84 <- st_transform(uk_grid_BNG, 4326)

# Plot the features
par(mar=c(0,0,0,0))
plot(uk_grid_WGS84, asp=1, border='grey', xlim=c(-13,4))
plot(st_geometry(uk_eire_sf), add=TRUE, border='darkgreen', lwd=2)
plot(uk_grid_BNG_as_WGS84, border='red', add=TRUE)

# use the project function, which gives us the choice of interpolating a representative value from the source data (method='bilinear') 
# or picking the cell value from the nearest neighbour to the new cell centre (method='near').
# Create the target raster
uk_raster_BNG <- rast(xmin=-200000, xmax=700000, ymin=0, ymax=1000000,
                      res=100000, crs='+init=EPSG:27700')
uk_raster_BNG_interp <- project(uk_raster_WGS84, uk_raster_BNG, method='bilinear')
uk_raster_BNG_near <- project(uk_raster_WGS84, uk_raster_BNG, method='near')
# Check and run
par(mfrow=c(1,2), mar=c(0,0,0,0))
plot(uk_raster_BNG_interp, main='Interpolated', axes=FALSE, legend=FALSE)
text(uk_raster_BNG_interp, digit=1)
plot(uk_raster_BNG_near, main='Nearest Neighbour',axes=FALSE, legend=FALSE)
text(uk_raster_BNG_near)

#####################################################
## Converting between vector and raster data types ##
#####################################################

## Vector to raster ##
# Converting vector data to a raster is a bit like reprojectRaster:
# you provide the target raster and the vector data and put it all through the rasterize function. 
# POINT: If a point falls anywhere within a cell, that value is assigned to the cell.
# LINESTRING: If any part of the linestring falls within a cell, that value is assigned to the cell.
# POLYGON: If the centre of the cell falls within a polygon, the value from that polygon is assigned to the cell.
# Create the target raster 
uk_20km <- rast(xmin=-200000, xmax=650000, ymin=0, ymax=1000000, 
                res=20000, crs='+init=EPSG:27700')

# Rasterizing polygons
uk_eire_poly_20km  <- rasterize(uk_eire_BNG, uk_20km, field='name')

plot(uk_eire_poly_20km)

# Getting raster versions of polygons is by far the most common use case, 
# but it is also possible to represent the boundary lines or even the polygon vertices as raster data.
# recast the polygon data, so that the rasterisation process knows to treat the data differently:
# the list of polygon vertices no longer form a closed loop (a polygon), but form a linear feature or a set of points.
# the sf package is the sheer quantity of warnings it will issue to avoid making errors. 
uk_eire_BNG$name <- as.factor(uk_eire_BNG$name)
st_agr(uk_eire_BNG) <- 'constant'

# Rasterizing lines.
uk_eire_BNG_line <- st_cast(uk_eire_BNG, 'LINESTRING')
uk_eire_line_20km <- rasterize(uk_eire_BNG_line, uk_20km, field='name')

# Rasterizing points 
# - This isn't quite as neat as there are two steps in the casting process:
#   Polygon -> Multipoint -> Point
uk_eire_BNG_point <- st_cast(st_cast(uk_eire_BNG, 'MULTIPOINT'), 'POINT')
uk_eire_point_20km <- rasterize(uk_eire_BNG_point, uk_20km, field='name')

# Plotting those different outcomes
# - Use the hcl.colors function to create a nice plotting palette
color_palette <- hcl.colors(6, palette='viridis', alpha=0.5)

# - Plot each raster
par(mfrow=c(1,3), mar=c(1,1,1,1))
plot(uk_eire_poly_20km, col=color_palette, legend=FALSE, axes=FALSE)
plot(st_geometry(uk_eire_BNG), add=TRUE, border='red')

plot(uk_eire_line_20km, col=color_palette, legend=FALSE, axes=FALSE)
plot(st_geometry(uk_eire_BNG), add=TRUE, border='red')

plot(uk_eire_point_20km, col=color_palette, legend=FALSE, axes=FALSE)
plot(st_geometry(uk_eire_BNG), add=TRUE, border='red')
# the differences between the polygon and line outputs. To recap:
# for polygons, cells are only included if the cell centre falls in the polygon, and
# for lines, cells are included if the line touches the cell at all.

## Raster to vector ##
# Converting a raster to vector data involves making a choice.
# A raster cell can either be viewed as a polygon with a value representing the whole cell or a point with the value representing the value at a specific location. 
# Note that it is uncommon to have raster data representing linear features and it is not trivial to turn raster data into LINESTRING vector data.

# The terra package provides functions to handle both of these: for polygons 
# whether to dissolve cells with identical values into larger polygons, or leave them all as individual cells.
# Get a set of dissolved polygons (the default) including NA cells
poly_from_rast <- as.polygons(uk_eire_poly_20km, na.rm=FALSE)

# Get individual cells (no dissolving)
cells_from_rast <- as.polygons(uk_eire_poly_20km, dissolve=FALSE)

# Get individual points
points_from_rast <- as.points(uk_eire_poly_20km)

#The terra package has its own format (SpatVector) for vector data, but it is easy to transform that back to the more familiar sf object,
# to see what the outputs contain:
#  the dissolved polygons have the original 6 features plus 1 new feature for the NA values,
#  the undissolved polygons and points both have 817 features - one for each grid cell in the raster that does not contain an NA value.
print(st_as_sf(poly_from_rast))
print(st_as_sf(cells_from_rast))
print(st_as_sf(points_from_rast))
# Plot the outputs - using key.pos=NULL to suppress the key
par(mfrow=c(1,3), mar=c(1,1,1,1))
plot(poly_from_rast, key.pos = NULL)
plot(cells_from_rast, key.pos = NULL)
plot(points_from_rast, key.pos = NULL, pch=4)

#########################
## Using data in files ##
#########################
# st_read function in sf --> vector
# rast() in terra --> raster 

## Saving vector data ##
st_write(uk_eire_sf, '../data/uk/uk_eire_WGS84.shp')
st_write(uk_eire_BNG, '../data/uk/uk_eire_BNG.shp')
# a shapefile is not a single file. A shapefile consists of a set of files: they all have the same name but different file suffixes
# and (at least) the files ending with .prj, .shp, .shx and .dbf, which is what st_write has created.

# GeoJSON stores the coordinates and attributes in a single text file: 
# it is technically human readable but you have to be familiar with JSON data structures.
# GeoPackage stores vector data in a single SQLite3 database file. 
# There are multiple tables inside this file holding various information about the data, but it is very portable and in a single file.
st_write(uk_eire_sf, '../data/uk/uk_eire_WGS84.geojson')
st_write(uk_eire_sf, '../data/uk/uk_eire_WGS84.gpkg')
# The sf package will try and choose the output format based on the file suffix (so .shp gives ESRI Shapefile)
st_write(uk_eire_sf, '../data/uk/uk_eire_WGS84.json', driver='GeoJSON')

## Saving raster data ##
# The GeoTIFF file format is one of the most common GIS raster data formats. 
#It is basically the same as a TIFF image file but contains embedded data describing the origin, resolution and coordinate reference system of the data. 
# Sometimes, a .tfw file: this is a ‘world’ file that contains the same information and should probably keep it with the TIFF file.
# Save a GeoTiff
writeRaster(uk_raster_BNG_interp, '../data/uk/uk_raster_BNG_interp.tif')
# Save an ASCII format file: human readable text. 
# Note that this format also creates an aux.xml and .prj file!
writeRaster(uk_raster_BNG_near, '../data/uk/uk_raster_BNG_ngb.asc', filetype='AAIGrid')

## Loading Vector data ##
# Load a vector shapefile
ne_110 <- st_read('../data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp')
# Also load some WHO data on 2016 life expectancy
# see: http://apps.who.int/gho/athena/api/GHO/WHOSIS_000001?filter=YEAR:2016;SEX:BTSX&format=csv
life_exp <- read.csv(file = "../data/WHOSIS_000001.csv")
# Generate two stacked plots with narrow margins
par(mfrow=c(2,1), mar=c(1,1,1,1))

# The first plot is easy
plot(ne_110['GDP_MD'],  asp=1, main='Global GDP', logz=TRUE, key.pos=4)

# Then for the second we need to merge the data
ne_110 <- merge(ne_110, life_exp, by.x='ISO_A3_EH', by.y='COUNTRY', all.x=TRUE)
# Create a sequence of break values to use for display
bks <- seq(50, 85, by=0.25)
# Plot the data
plot(ne_110['Numeric'], asp=1, main='Global 2016 Life Expectancy (Both sexes)',
     breaks=bks, pal=hcl.colors, key.pos=4)

## Loading XY data ##
# Read in Southern Ocean example data
so_data <- read.csv('../data/Southern_Ocean.csv', header=TRUE)

# Convert the data frame to an sf object
so_data <- st_as_sf(so_data, coords=c('long', 'lat'), crs=4326)
print(so_data)

## Loading Raster data ##
etopo_25 <- rast('../data/etopo_25.tif')
# Look at the data content
print(etopo_25)
# Plot it 
plot(etopo_25, plg=list(ext=c(190, 200, -90, 90)))
# controlling raster plots
bks <- seq(-10000, 6000, by=250)
land_cols  <- terrain.colors(24)
sea_pal <- colorRampPalette(c('darkslateblue', 'steelblue', 'paleturquoise'))
sea_cols <- sea_pal(40)
plot(etopo_25, axes=FALSE, breaks=bks, 
     col=c(sea_cols, land_cols), type='continuous',  
     plg=list(ext=c(190, 200, -90, 90))
) 

## Raster Stacks ##
# use the geodata package to get some data with monthly data bands. 
# Download bioclim data: global maximum temperature at 10 arc minute resolution
tmax <- worldclim_global(var='tmax', res=10, path='data')
# The data has 12 layers, one for each month
print(tmax)
# access different layers using [[
# Extract  January and July data and the annual maximum by location.
tmax_jan <- tmax[[1]]
tmax_jul <- tmax[[7]]
tmax_max <- max(tmax)

# Plot those maps
par(mfrow=c(2,2), mar=c(2,2,1,1))
bks <- seq(-50, 50, length=101)
pal <- colorRampPalette(c('lightblue','grey', 'firebrick'))
cols <- pal(100)
plg <- list(ext=c(190, 200, -90, 90))

plot(tmax_jan, col=cols, breaks=bks, 
     main='January maximum temperature', type='continuous', plg=plg)
plot(tmax_jul, col=cols, breaks=bks, 
     main='July maximum temperature', type='continuous', plg=plg)
plot(tmax_max, col=cols, breaks=bks, 
     main='Annual maximum temperature', type='continuous', plg=plg)


#######################################
## Overlaying raster and vector data ##
#######################################
# use some data to build up a more complex map of chlorophyll concentrations in the Southern Ocean.

## Cropping data ##
# When only interested in a subset of the area covered by a GIS dataset. 
# Cropping the data to the area of interest can make plotting easier and can also make GIS operations a lot faster, particularly if the data is complex.
so_extent <- ext(-60, -20, -65, -45)

# The crop function for raster data...
so_topo <- crop(etopo_25, so_extent)

# ... and the st_crop function to reduce some higher resolution coastline data
ne_10 <- st_read('../data/ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp')
st_agr(ne_10) <- 'constant'
so_ne_10 <- st_crop(ne_10, so_extent)

# plotting Sourthern Ocean chlorophyII(叶绿素)
sea_pal <- colorRampPalette(c('grey30', 'grey50', 'grey70'))

# Plot the underlying sea bathymetry
plot(so_topo, col=sea_pal(100), asp=1, legend=FALSE)
contour(so_topo, levels=c(-2000, -4000, -6000), add=TRUE, col='grey80')

# Add the land
plot(st_geometry(so_ne_10), add=TRUE, col='khaki', border='grey40')

#############################################
## Spatial joins and raster data extraction##
#############################################

## Spatial joining ##
# Spatial join means merge data spatially, similar as merge data with sf object by matching values in columns

# mapping ‘mosquito outbreaks’ in Africa: we are actually going to use some random data, mostly to demonstrate the useful st_sample function. 
set.seed(1)
# extract Africa from the ne_110 data and keep the columns we want to use
africa <- subset(ne_110, CONTINENT=='Africa', select=c('ADMIN', 'POP_EST'))

# transform to the Robinson projection
africa <- st_transform(africa, crs='ESRI:54030')
# create a random sample of points
mosquito_points <- st_sample(africa, 1000)

# Create the plot
plot(st_geometry(africa), col='khaki')
plot(mosquito_points, col='firebrick', add=TRUE)
# Show the sampling sites
plot(st_geometry(so_data), add=TRUE, pch=4, cex=2, col='white', lwd=3)

# turn the mosquito_points object from a geometry column (sfc) into a full sf data frame to join the data
mosquito_points <- st_sf(mosquito_points)
mosquito_points <- st_join(mosquito_points, africa['ADMIN'])

plot(st_geometry(africa), col='khaki')

# Add points coloured by country
plot(mosquito_points['ADMIN'], add=TRUE)

# aggregate the points within countries
mosquito_points_agg <- aggregate(
  mosquito_points, 
  by=list(country=mosquito_points$ADMIN), FUN=length
)
names(mosquito_points_agg)[2] <-'n_outbreaks'
print(mosquito_points_agg)
# Merge the number of outbreaks back onto the sf data
africa <- st_join(africa, mosquito_points_agg)
africa$area <- as.numeric(st_area(africa))
print(africa)
# Plot the results
par(mfrow=c(1,2), mar=c(3,3,1,1), mgp=c(2,1, 0))
plot(n_outbreaks ~ POP_EST, data=africa, log='xy', 
     ylab='Number of outbreaks', xlab='Population size')
plot(n_outbreaks ~ area, data=africa, log='xy',
     ylab='Number of outbreaks', xlab='Area (m2)')


# Alien invasion -- produce the map countries at risk
# Load the data and convert to a sf object
alien_xy <- read.csv('../data/aliens.csv')
alien_xy <- st_as_sf(alien_xy, coords=c('long','lat'), crs=4326)

# Add country information and find the total number of aliens per country
alien_xy <- st_join(alien_xy, ne_110['ADMIN'])
aliens_by_country <- aggregate(n_aliens ~ ADMIN, data=alien_xy, FUN=sum)

# Add the alien counts into the country data 
ne_110 <- merge(ne_110, aliens_by_country, all.x=TRUE)
ne_110$people_per_alien <- with(ne_110,  POP_EST / n_aliens )

# Find which countries are in danger
ne_110$in_danger <- ne_110$people_per_alien < 1000

# Plot the danger map
plot(ne_110['in_danger'], pal=c('grey', 'red'), key.pos=4)


## Extracting data from Rasters ##
# extract data from a raster dataset in certain locations
# Examples include to know the exact altitude or surface temperature of sampling sites or average values within a polygon

# use a chunk of the full resolution ETOPO1 elevation data
uk_eire_etopo <- rast('../data/uk/etopo_uk.tif')

uk_eire_detail <- subset(ne_10, ADMIN %in% c('United Kingdom', "Ireland"))
uk_eire_detail_raster <- rasterize(uk_eire_detail, uk_eire_etopo)
uk_eire_elev <- mask(uk_eire_etopo, uk_eire_detail_raster)

par(mfrow=c(1,2), mar=c(3,3,1,1), mgp=c(2,1,0))
plot(uk_eire_etopo, plg=list(ext=c(3,4, 50, 59)))
plot(uk_eire_elev, plg=list(ext=c(3,4, 50, 59)))
plot(st_geometry(uk_eire_detail), add=TRUE, border='grey')


## Raster cell statistics and locations ##
# The global function provides a way to find global summary statistics of the data in a raster. 
# We can also find out the locations of cells with particular characteristics using where.max of where.min. 
# Both of those functions return cell ID numbers, but the xyFromCell allows you to turn those ID numbers into coordinates.
uk_eire_elev >= 1195

global(uk_eire_elev, max, na.rm=TRUE)
global(uk_eire_elev, quantile, na.rm=TRUE)

# Which is the highest cell
where.max(uk_eire_elev)

# Which cells are above 1100m
high_points <- where.max(uk_eire_elev >= 1100, value=FALSE)
xyFromCell(uk_eire_elev, high_points[,2])

# Highlight highest point and areas below sea level
# Plot the locations of the maximum altitude and cells below sea level on the map. 
max_cell <- where.max(uk_eire_elev)
max_xy <- xyFromCell(uk_eire_elev, max_cell[2])
max_sfc<- st_sfc(st_point(max_xy), crs=4326)

bsl_cell <- where.max(uk_eire_elev < 0, values=FALSE)
bsl_xy <- xyFromCell(uk_eire_elev, bsl_cell[,2])
bsl_sfc <- st_sfc(st_multipoint(bsl_xy), crs=4326)

plot(uk_eire_elev)
plot(max_sfc, add=TRUE, pch=24, bg='red')
plot(bsl_sfc, add=TRUE, pch=25, bg='lightblue', cex=0.6)

## The extract function ##
# POINT: extract the values under the points.
# LINESTRING: extract the values under the linestring
# POLYGON: extract the values within the polygon

# Extracting raster values under points is really easy.
uk_eire_capitals$elev <- extract(uk_eire_elev, uk_eire_capitals, ID=FALSE)
print(uk_eire_capitals)

# the extract function for polygons returns a data frame of individual raster cell values within each polygon, along with an ID code showing the polygon ID:
etopo_by_country <- extract(uk_eire_elev, uk_eire_sf['name'])
head(etopo_by_country)

# do summary statistics across those values:
aggregate(etopo_uk ~ ID, data=etopo_by_country, FUN='mean', na.rm=TRUE)

# use the zonal function to specify a summary statistic that should be calculated within polygons. 
zones <- rasterize(st_transform(uk_eire_sf, 4326), uk_eire_elev, field='name')
etopo_by_country <- zonal(uk_eire_elev, zones, fun='mean', na.rm=TRUE)

print(etopo_by_country)

# One feature of GPX files is that they contain multiple layers: 
# essentially different GIS datasets within a single source. 
# The st_layers function allows us to see the names of those layers so we can load the one we want.
st_layers('../data/uk/National_Trails_Pennine_Way.gpx')
# load the data, showing off the ability to use SQL queries to load subsets of the data
pennine_way <- st_read('../data/uk/National_Trails_Pennine_Way.gpx',
                       query="select * from routes where name='Pennine Way'")

# Reproject the Penine Way
# reproject the vector data
pennine_way_BNG <- st_transform(pennine_way, crs=27700)
# create the target raster and project the elevation data into it.
bng_2km <- rast(xmin=-200000, xmax=700000, ymin=0, ymax=1000000, 
                res=2000, crs='EPSG:27700')
uk_eire_elev_BNG <- project(uk_eire_elev, bng_2km, method='cubic')

# Simplify the data
pennine_way_BNG_simple <- st_simplify(pennine_way_BNG,  dTolerance=100)

# Zoom in to the whole route and plot the data
par(mfrow=c(1,2), mar=c(1,1,1,1))

plot(uk_eire_elev_BNG, xlim=c(3e5, 5e5), ylim=c(3.8e5, 6.3e5),
     axes=FALSE, legend=FALSE)
plot(st_geometry(pennine_way_BNG), add=TRUE, col='black')
plot(st_geometry(pennine_way_BNG_simple), add=TRUE, col='darkred')

# Add a zoom box and use that to create a new plot
zoom <- ext(3.78e5, 3.84e5, 4.72e5, 4.80e5)
plot(zoom, add=TRUE, border='red')

# Zoomed in plot
plot(uk_eire_elev_BNG, ext=zoom, axes=FALSE, legend=FALSE)
plot(st_geometry(pennine_way_BNG), add=TRUE, col='black')
plot(st_geometry(pennine_way_BNG_simple), add=TRUE, col='darkred')

#  extract the elevations, cell IDs and the XY coordinates of cells falling under that route. 
# Extract the data
pennine_way_trans <- extract(uk_eire_elev_BNG, pennine_way_BNG_simple, xy=TRUE)
head(pennine_way_trans)
# Now we can use Pythagoras to find the distance along the transect
pennine_way_trans$dx <- c(0, diff(pennine_way_trans$x))
pennine_way_trans$dy <- c(0, diff(pennine_way_trans$y))
pennine_way_trans$distance_from_last <- with(pennine_way_trans, sqrt(dx^2 + dy^2))
pennine_way_trans$distance <- cumsum(pennine_way_trans$distance_from_last) / 1000

plot( etopo_uk ~ distance, data=pennine_way_trans, type='l', 
      ylab='Elevation (m)', xlab='Distance (km)')


###################
## Mini Projects ##
###################
## WGS84 coordinates of a transect through New Guinea ##
transect_long <- c(132.3, 135.2, 146.4, 149.3)
transect_lat <- c(-1, -3.9, -7.7, -9.8)
# Create a total annual precipitation transect for New Guinea #
# Get the precipitation data
ng_prec <- worldclim_tile(var='prec', res=0.5, lon=140, lat=-10, path='data')

# Reduce to the extent of New Guinea - crop early to avoid unnecessary processing!
ng_extent <- ext(130, 150, -10, 0)
ng_prec <- crop(ng_prec, ng_extent)

# Calculate annual precipitation
ng_annual_prec <- sum(ng_prec)

# Now reproject to UTM 54S. The code here is using reprojecting the extent of the
# raster data to get sensible values for the UTM 54S extent. We are then picking extent 
# values here that create a neat 1000m grid with sensible cell edges
ng_extent_poly <- st_as_sfc(st_bbox(ng_extent, crs=4326))
ng_extent_utm <- ext(-732000, 1506000, 8874000, 10000000)

# Create the raster and reproject the data
ng_template_utm <- rast(ng_extent_utm, res=1000, crs="+init=EPSG:32754")
ng_annual_prec_utm <- project(ng_annual_prec, ng_template_utm)

# Create and reproject the transect and then segmentize it to 1000m
transect <-  st_linestring(cbind(x=transect_long, y=transect_lat))
transect <- st_sfc(transect, crs=4326)
transect_utm <- st_transform(transect, crs=32754)
transect_utm <- st_segmentize(transect_utm, dfMaxLength=1000)

# Extract the transect data
transect_data <- extract(ng_annual_prec_utm, st_sf(transect_utm), xy=TRUE)

# Now we can use Pythagoras to find the distance along the transect
transect_data$dx <- c(0, diff(transect_data$x))
transect_data$dy <- c(0, diff(transect_data$y))
transect_data$distance_from_last <- with(transect_data, sqrt(dx^2 + dy^2))
transect_data$distance <- cumsum(transect_data$distance_from_last) / 1000

# Get the natural earth high resolution coastline.
ne_10_ng  <- st_crop(ne_10, ng_extent_poly)
ne_10_ng_utm <-  st_transform(ne_10_ng, crs=32754)

par(mfrow=c(2,1), mar=c(3,3,1,1), mgp=c(2,1,0))
plot(ng_annual_prec_utm, plg=list(ext=c(1700000, 1800000, 8950000, 9950000)))

plot(st_geometry(ne_10_ng_utm), add=TRUE, col=NA, border='grey50')
plot(transect_utm, add=TRUE)

par(mar=c(3,3,1,1))
plot( sum ~ distance, data=transect_data, type='l', 
      ylab='Annual precipitation (mm)', xlab='Distance (km)')

## Fishing pressure in Fiji ##
# Researchers have identified 7 commonly used fishing sites around the island of Kadavu in Fiji. 
# The have also conducted surveys of the coastal villages known to use these sites and are trying to identify how many households are likely to use site. 
# We are going to use the simplifying assumption that each village will always use the closest site.

# Loading the data #
# Download the GADM data for Fiji, convert to sf and then extract Kadavu
fiji <- gadm(country='FJI', level=2, path='data/fiji')
fiji <- st_as_sf(fiji)
kadavu <- subset(fiji, NAME_2 == 'Kadavu')

# Load the villages and sites and convert to sf
villages <- readWorkbook('../data/fiji/FishingPressure.xlsx', 'Villages')
villages <- st_as_sf(villages, coords=c('long','lat'), crs=4326)
sites <- readWorkbook('../data/fiji/FishingPressure.xlsx', 'Field sites', startRow=3)
sites <- st_as_sf(sites, coords=c('Long','Lat'), crs=4326)

# Reproject the data UTM60S
kadavu <- st_transform(kadavu, 32760)
villages <- st_transform(villages, 32760)
sites <- st_transform(sites, 32760)


# Map to check everything look right.
plot(st_geometry(sites), axes=TRUE, col='blue', pch=4)
plot(st_geometry(villages), add=TRUE, col='red')
plot(st_geometry(kadavu), add=TRUE)


# Create the cost surface #
# Create a template raster covering the whole study area, at a given resolution
res <- 100
r <- rast(xmin=590000, xmax=670000, ymin=7870000, ymax=7940000, crs='EPSG:32760', res=res)

# Rasterize the island as a POLYGON to get cells that cannot be traversed
kadavu_poly <- rasterize(kadavu, r, field=1, background=0)

# Rasterize the island as a MULTILINESTRING to get the coastal 
# cells that _can_ be traversed
coast <- st_cast(kadavu, 'MULTILINESTRING')
kadavu_lines <- rasterize(coast, r, field=1, background=0)

# Combine those to give cells that are in the sea (kadavu_poly=0) or 
# on the coast (kadavu_lines=1)
sea_r <- (! kadavu_poly) | kadavu_lines

# Set the costs
sea_r[sea_r == 0] <- NA
sea_r[! is.na(sea_r)] <- 1

# Plot the map and then zoom in to show that the coastal cells can
# be travelled through
par(mfrow=c(1,2), mar=c(2,2,1,1))
plot(sea_r, col='lightblue')
zoom <- ext(599000, 602000, 7884000, 7887000)
plot(zoom, add=TRUE)

plot(sea_r, ext=zoom, col='lightblue')
plot(st_geometry(kadavu), add=TRUE)

# Finding launch points #
# Find the nearest points on the coast to each village
village_coast <- st_nearest_points(villages, coast)

# Extract the end point on the coast and convert from MULTIPOINT to POINT
launch_points <- st_line_sample(village_coast, sample=1)
launch_points <- st_cast(launch_points, 'POINT')

# Zoom in to a bay on Kadavu
par(mar=c(0,0,0,0))
plot(st_geometry(kadavu), xlim=c(616000, 618000), ylim=c(7889000, 7891000), col='khaki')
# Plot the villages, lines to the nearest coast and the launch points.
plot(st_geometry(villages), add=TRUE, col='firebrick', lwd=2)
plot(village_coast, add=TRUE, col='black')
plot(launch_points, add=TRUE, col='darkgreen', lwd=2)
# add the launch points in to our villages object
villages$launch_points <- launch_points
st_geometry(villages) <- 'launch_points'

# Find distance #
# how the calculation works - note the added costs of moving around the blocking cell into the bottom right corner.
r <- rast(xmin=0, ymin=0, xmax=50, ymax=50, res=10, crs='EPSG:32760')

# Set cell values:
values(r) <- 1  # Set all cells to be non-NA
r[3,3] <- 0     # This is a target cell
r[4,4] <- NA    # Set one NA cell

# Calculate and plot distances
d <- gridDistance(r)
plot(d, legend=NULL)
text(d, digits=1)

# Scaling that up to give an example for a single site:
# Make a copy of the sea map
dist <- sea_r

# Find the location of a site and make that the target
site_idx <- 49
village_cell <- cellFromXY(dist, st_coordinates(villages[site_idx,]))
dist[village_cell] <- 0

# Now we can calculate the cost distance for each launch point to each site...
costs <- gridDistance(dist)

plot(costs, plg=list(ext=c(672000, 674000, 7870000, 7940000)))
plot(st_geometry(villages[site_idx,]), add=TRUE, pch=4)
plot(st_geometry(sites), add=TRUE)

# And grab the costs at each fishing site
distances_to_site <- extract(costs, sites)
print(distances_to_site)

# and find the smallest distance
nearest_site <- which.min(distances_to_site$layer)

# Loop over sites #
# to wrap that process in a loop to find the nearest site for each village 
# Create fields to hold the nearest fishing site data
villages$nearest_site_index <- NA
villages$nearest_site_name <- NA

# Loop over the sites
for (site_idx in seq(nrow(villages))) {
  
  # Make a copy of the sea map
  dist <- sea_r
  
  # Find the location of a site and make that the target
  village_cell <- cellFromXY(dist, st_coordinates(villages[site_idx,]))
  dist[village_cell] <- 0
  
  # Now we can calculate the cost distance for each launch point to each site...
  costs <- gridDistance(dist)
  
  # And find the nearest site
  distances_to_site <- extract(costs, sites)
  nearest_site <- which.min(distances_to_site$layer)
  
  # Find the index and name of the lowest distance in each row
  villages$nearest_site_index[site_idx] <- nearest_site
  villages$nearest_site_name[site_idx] <- sites$Name[nearest_site]
  
}

# work out the fishing load for each site and map which villages prefer which site:
# Find the total number of buildings  per site and merge that data
# into the sites object
site_load <- aggregate(building_count ~ nearest_site_name, data=villages, FUN=sum)
sites_with_load <- merge(sites, site_load, by.x='Name', by.y='nearest_site_name', all.x=TRUE)

# Now build up a complex plot
par(mar=c(0,0,0,0))
plot(st_geometry(kadavu))

# add the villages, colouring by nearest site and showing the village 
# size using the symbol size (cex)
plot(villages['nearest_site_name'], add=TRUE, pch=20, cex=log10(villages$building_count))

# Add the sites
plot(st_geometry(sites_with_load), add=TRUE, col='red', pch=4, lwd=4)
print(sites_with_load)

################################
## Using ggplot to make maps ##
################################
library(ggplot2)
ggplot(ne_110) +
  geom_sf() +
  theme_bw()
# There are several ggplot extensions for sf that make it easier to colour and label your ggplot maps. 
# !!! Here is a bad example !!!:
europe <- st_crop(ne_110, ext(-10,40,35,75))
ggplot(europe) +
  geom_sf(aes(fill=GDP_MD)) +
  scale_fill_viridis_c() +
  theme_bw() + 
  geom_sf_text(aes(label = ADMIN), colour = "white")

# European life expectancy #
# Calculate the extent in the LAEA projection of the cropped data
europe_crop_laea <- st_transform(europe, 3035)

# Reproject all of the country data and _then_ crop to the previous extent
europe_laea <- st_transform(ne_110, 3035)
europe_laea <- st_crop(europe_laea, europe_crop_laea)

# Plot the two maps
p1 <- ggplot(europe_crop_laea) +
  geom_sf(aes(fill=log(GDP_MD))) +
  scale_fill_viridis_c() +
  theme_bw() + 
  theme(legend.position="bottom") +
  geom_sf_text(aes(label = ADM0_A3), colour = "grey20")

p2 <- ggplot(europe_laea) +
  geom_sf(aes(fill=log(GDP_MD))) +
  coord_sf(expand=FALSE) +
  scale_fill_viridis_c() +
  theme_bw() + 
  theme(legend.position="bottom") +
  geom_sf_text(aes(label = ADM0_A3), colour = "grey20")

library(gridExtra)
grid.arrange(p1, p2, ncol=2)

#####################
## Colour palettes ##
#####################
## Viridis
## Brewer
library(RColorBrewer)
display.brewer.all()
# The brewer packages has a neat way of showing colours that are colourblind-friendly.
display.brewer.all(colorblindFriendly = TRUE)

**********************************************************************

Testing GISPractical1.R...

Output (only first 500 characters): 


**********************************************************************

**********************************************************************

Encountered error (or warning):

***IGNORE IF THIS ERROR IS EXPECTED AS PART OF AN IN-CLASS EXERCISE***

Error: package or namespace load failed for ‘terra’ in .doLoadActions(where, attach):
 error in load action .__A__.1 for package terra: loadModule(module = "spat", what = TRUE, env = ns, loadNow = TRUE): Unable to load module "spat": function 'Rcpp_precious_remove' not provided by package 'Rcpp'
Execution halted

======================================================================
Inspecting script file GISPractical3.R...

File contents are:

**********************************************************************
################################################################################
#####################                                       ####################
#################### Practical Three: Spatial Modelling in R ###################
#####################                                       ####################
################################################################################
## Author: Anqi Wang 
##
## Date Created: Oct, 2022
##
## Email: aw222@ic.ac.uk

# Load the packages
library(ncf)
library(raster)
library(sf)
library(spdep)
library(SpatialPack)
library(spatialreg)
library(nlme)
library(spgwr)
library(spmoran)

##################
## The dataset ##
##################

**********************************************************************

Testing GISPractical3.R...

Output (only first 500 characters): 


**********************************************************************

**********************************************************************

Encountered error (or warning):

***IGNORE IF THIS ERROR IS EXPECTED AS PART OF AN IN-CLASS EXERCISE***

Error in library(ncf) : there is no package called ‘ncf’
Execution halted

======================================================================
======================================================================
Finished running scripts

Ran into 4 errors

======================================================================
======================================================================

FINISHED WEEKLY ASSESSMENT

Current Points for the Week = 100

NOTE THAT THESE ARE POINTS, NOT MARKS FOR THE WEEK!