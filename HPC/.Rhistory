vec <- c(vec, species_richness(community))
}
return(vec)
}
# Question 12
# Explain what you found from this plot about the effect of initial conditions.
# Why does the neutral model simulation give you those particular results
question_12 <- function()  {
graphics.off()  # clear any existing graphs
png(filename="question_12.png", width = 600, height = 400) # save the .png file
# plot your graph here
Sys.sleep(0.1)
y_max <- neutral_time_series_speciation(community = init_community_max(100), 0.1, 200)
y_min <- neutral_time_series_speciation(community = init_community_min(100), 0.1, 200)
x <- c(1:201)
plot(x, y_max, main="Time Series Graph of Neutral Model", ylab="Species Richness", xlab="Generations", type="l", col="red")
lines(x, y_min, col="blue", type="l")
legend(x = 120, y = 100, legend = c("Initial Maximum Population", "Initial Minimum Population"), col = c("red","blue"),lty=1:2, cex=0.8)
dev.off()
return("When a community is simulated by the neutral model with the same rate of speciation, there is likely to have a dynamic equilibrium.
There is no obvious impact of the initial states of richness, because the chances of extinction and speciation are the same in both populations.
However, the model with speciations prevents the mono-dominance from persisting as new species can occur in both scenarios.
Regardless of initial values, evaluate the speciation rate and numbers of individuals, and they would converge to a dynamic equilibrium with similar pattern.")
}
# Question 13
species_abundance <- function(community)  {
abundance <- as.numeric(sort(table(community), decreasing = TRUE))
return(abundance)
}
# Question 14
octaves <- function(abundance_vector) {
OctVet <- tabulate(floor(log2(abundance_vector))+1) # +1 ensures that 0's are read while log groups them
return(OctVet)
}
# Question 15
# return the sum of x and y
# correct the zero length
sum_vect <- function(x, y) {
length_diff <- length(x) - length(y)
if (length_diff > 0){
y <- c(y, rep(0, length_diff))
}
if (length_diff < 0){
x <- c(x, rep(0, abs(length_diff)))
}
x_y_sum <- x + y
return(x_y_sum)
}
# Question 16
# bar plot of species abundance distributions
# speciation rate = 0.1, community size =100, period = 200 generation
# continue the simulation from where left off for 2000 generations
# for every 20 generations, record the species abundance octave vector
# no input requires
question_16 <- function() {
graphics.off()  # clear any existing graphs
# initial parameters
Max_com <- init_community_max(100)
Min_com <- init_community_min(100)
for (i in 1:200){ # burn-in period
#returning the species at the end of each generation
burn_in_max <- neutral_generation_speciation(Max_com, speciation_rate = 0.1)
burn_in_min <- neutral_generation_speciation(Min_com, speciation_rate = 0.1)
# the next iteration should be based of t-1 generation -- replace comm
Max_com <- burn_in_max
Min_com <- burn_in_min
}
# record the species abundance octave vector
Max_oct <- octaves(species_abundance(Max_com))
Min_oct <- octaves(species_abundance(Min_com))
for (i in 1:2000){ # continues the simulation for further 2000 generations
# for each run of the loop, add the current octave to the octave previously recorded
Max_com <- neutral_generation_speciation(Max_com, speciation_rate = 0.1)
Min_com <- neutral_generation_speciation(Min_com, speciation_rate = 0.1)
if (i %% 20 == 0){ # every 20 generation
Max_oct <- sum_vect(Max_oct, octaves(species_abundance(Max_com)))
Min_oct <- sum_vect(Min_oct, octaves(species_abundance(Min_com)))
}
}
# calculate the mean
Max_oct_vect <- Max_oct/100
Min_oct_vect <- Min_oct/100
# plot for min
png(filename="question_16_min.png", width = 600, height = 400)
# plot your graph here
Sys.sleep(0.1)
barplot(Min_oct_vect,
main="Species Abundance Octave Vector of Min",
xlab = "Species Abundance Octave",
ylab = "Mean Species Abundance Distribution", # mean octaves
cex.main=0.8)
dev.off()
# plot for max
png(filename="question_16_max.png", width = 600, height = 400)
# plot your graph here
Sys.sleep(0.1)
barplot(Max_oct_vect,
main="Species Abundance Octave Vector of Max",
xlab = "Species Abundance Octave",
ylab = "Mean Species Abundance Distribution", # mean octaves
cex.main=0.8)
dev.off()
return("The initial richness of the system does not affect the final results.
As the speciation rate is equal to all the simulated community in the neutral model, the species abundance follows a Poisson distribution shape.
Therefore, no matter the initial richness is, the system tends to have the same trends and get a dynamic equilibrium.")
}
# Question 17
# input: init_community_min
# wall_time <- proc.time(speciation_rate)
# interval_rich <- burn_in period (burn_in_generations)
# species richness recorded in a vector -> time_series (%% + loop)
# (list) -> interval_oct generation
# save results in output_file_name <- time_seies of species richness recorded during burn_in_generations
# abundance_list <- the list of species abundance octaves
# test with: neutral_cluster_run(speciation_rate = 0.1, size = 100, wall_time = 10, interval_rich = 1, interval_oct = 10, burn_in_generations = 200, output_file_name = "my_test_file_1.rda")
neutral_cluster_run <- function(speciation_rate, size, wall_time, interval_rich, interval_oct, burn_in_generations, output_file_name) {
community <- init_community_min(size) # set initial community size
start_timer <- as.numeric(proc.time()[3]) # function will run until wall_time has elapsed, measured in minutes
time_series <- c() # species richness in vector
SpAbd_octaves <- list() # empty list to store the entire simulated octave
#final_com <- c() # vector for final community constitute
count = 1 #count from the first generation
while ((as.numeric(proc.time()[3]) - start_timer) <= (wall_time*60)) { # run the loop and convert the time unit
community <- neutral_generation_speciation(community, speciation_rate)
if (count <= burn_in_generations){
if(count %% interval_rich == 0 ) { # only record during the burn_in_generation
time_series <- c(species_richness(community), time_series)
}
}
if (count %% interval_oct == 0) { # record species abundances as octaves
o_vect <- octaves(species_abundance(community))
SpAbd_octaves <- append(SpAbd_octaves, list(o_vect)) # adds into list
}
count <- count + 1
}
save(time_series, SpAbd_octaves, community, speciation_rate,
size, wall_time, interval_rich, interval_oct, burn_in_generations,
file = output_file_name)
return(paste(output_file_name, "done!", sep=" "))
}
# Question 20
# read and process the output files
# only use the data of the abundance octaves after the burn-in time
# calculate a mean value across all the saved data for each abundance octaves and for each community size
# save all results in .rda file as a list of four vectors
process_neutral_cluster_results <- function() {
# clear all exist graphs
graphics.off()
files <- list.files(pattern = "Simulation_*") # creates list of files output from HPC
Com_size_500 <- list() # generates empty lists for each different community size used in the HPC cluster
Com_size_1000 <- list()
Com_size_2500 <- list()
Com_size_5000 <- list()
Com_size_500_length <- c(0) # creates empty vectors to count the total number of octaves recorded for each community size during HPC run
Com_size_1000_length <- c(0)
Com_size_2500_length <- c(0)
Com_size_5000_length <- c(0)
for(i in 1:length(files)){ #loop through all files
load(files[i]) # load each file one by one
if (size == 500){ # if the community size is 500;
Com_size_500 <- c(Com_size_500, SpAbd_octaves[81:length(SpAbd_octaves)]) # add all the octaves from this file, apart from those recorded during burn in, to the octave vector created for community sizes of 500
Com_size_500_length <- (Com_size_500_length + (length(SpAbd_octaves)-80)) # sums the amount of octaves recorded for all runs for community sizes of 500
} # below code does same as above for different community sizes
if (size == 1000){
Com_size_1000 <- c(Com_size_1000, SpAbd_octaves[81:length(SpAbd_octaves)])
Com_size_1000_length <- (Com_size_1000_length + (length(SpAbd_octaves)-80))
}
if (size == 2500){
Com_size_2500 <- c(Com_size_2500, SpAbd_octaves[81:length(SpAbd_octaves)])
Com_size_2500_length <- (Com_size_2500_length + (length(SpAbd_octaves)-80))
}
if (size == 5000){
Com_size_5000 <- c(Com_size_5000, SpAbd_octaves[81:length(SpAbd_octaves)])
Com_size_5000_length <- (Com_size_5000_length + (length(SpAbd_octaves)-80))
}
}
Mean_size_500 <- Com_size_500[[1]]
for (i in 1:length(Com_size_500)){
Mean_size_500 <- sum_vect(Mean_size_500, Com_size_500[[i]])
} # iterates through octaves list and sums all items
Mean_size_500 <- Mean_size_500/Com_size_500_length # finds mean abundance octaves for all simulations with community size of 500
Mean_size_1000 <- Com_size_1000[[1]]
for (i in 1:length(Com_size_1000)){
Mean_size_1000 <- sum_vect(Mean_size_1000, Com_size_1000[[i]])
}
Mean_size_1000 <- Mean_size_1000/Com_size_1000_length
Mean_size_2500 <- Com_size_2500[[1]]
for (i in 1:length(Com_size_2500)){
Mean_size_2500 <- sum_vect(Mean_size_2500, Com_size_2500[[i]])
}
Mean_size_2500 <- Mean_size_2500/Com_size_2500_length
Mean_size_5000 <- Com_size_5000[[1]]
for (i in 1:length(Com_size_5000)){
Mean_size_5000 <- sum_vect(Mean_size_5000, Com_size_5000[[i]])
} # iterates through octaves list and sums all items
Mean_size_5000 <- Mean_size_5000/Com_size_5000_length
combined_results <- list(Mean_size_500, Mean_size_1000, Mean_size_2500, Mean_size_5000) #create your list output here to return
save(combined_results, file = "process_neutral_cluster_results.rda")# save results to an .rda file
return("TOTAL OUTPUT FILE IS DONE!!!")
}
plot_neutral_cluster_results <- function(){
# load combined_results from your .rda file
load("process_neutral_cluster_results.rda")
Com500 <- combined_results[[1]]
Com1000 <- combined_results[[2]]
Com2500 <- combined_results[[3]]
Com5000 <- combined_results[[4]]
# Plot
png(filename="plot_neutral_cluster_results.png", width = 600, height = 400)
# plot your graph here
Sys.sleep(0.1)
par(mfrow = c(2,2), las = 2) # 2 columns and 2 rows of graphs to be plotted
# plot mean abundance octaves for each community size
barplot(Com500, main = "Community size = 500", xlab = "Species Abundance Octave", ylab = "Frequency")
barplot(Com1000, main = "Community size = 1000", xlab = "Species Abundance Octave", ylab = "Frequency")
barplot(Com2500, main = "Community size = 2500", xlab = "Species Abundance Octave", ylab = "Frequency")
barplot(Com5000, main = "Community size = 5000", xlab = "Species Abundance Octave", ylab = "Frequency")
title("Mean Species Abundance Octaves for Neutral Model Simulated on HPC (Speciation_rate = 0.64)", outer = T, line = -1)
dev.off()
return("THE PLOT IS DONE!!!")
}
# Question 21
state_initialise_adult <- function(num_stages,initial_size){
# Number of individuals in adult stages and total number of life stages in the model
return(c(rep(0,num_stages-1),initial_size))
# Create a 0 vector concatenating with the initial adult population size to create the final state vector
}
# Question 22
state_initialise_spread <- function(num_stages, initial_size) {
# Create a 0 vector with length(num_stages)
state_spread <- rep(0, num_stages)
for (index in seq(num_stages)) {
state_spread[index] <- floor(initial_size / num_stages)
}
rest_pop <- initial_size %% num_stages
if (rest_pop == 0) {
# if this can be evenly disvisible, then just return the state
return(state_spread)
}
for (index in seq(rest_pop)) {
state_spread[index] <- state_spread[index] +1
}
return(state_spread)
}
# Question 23
deterministic_step <- function(state, projection_matrix){
# Calculate the new state by performing a matrix multiplication
# between the projection matrix and the state vector
new_state <- projection_matrix %*% state
return(new_state)
}
# Question 24
deterministic_simulation <- function(initial_state, projection_matrix, simulation_length) {
# Create a vector to store the population size at each time step
population_size <- rep(NA, simulation_length+1)
population_size[1] <- sum(initial_state)
# Loop through the time steps and apply the deterministic model
for (t in 1:simulation_length) {
state <- deterministic_step(initial_state, projection_matrix)
initial_state <- state
population_size[t+1] <- sum(state)
}
# Return the population size vector
return(population_size)
}
# Question 25
# Model a population with four life stages and to plot a time series of population size
question_25 <- function(){
# Set the simulation length and projection matrix
simulation_length <- 24
projection_matrix <- matrix(c(0.1, 0.6, 0.0, 0.0,
0.0, 0.4, 0.4, 0.0,
0.0, 0.0, 0.7, 0.25,
2.6, 0.0, 0.0, 0.4),nrow=4,ncol=4)
# Initial conditions
initial_state_1 <- state_initialise_adult(4, 100)
initial_state_2 <- state_initialise_spread(4, 100)
# Simulate the time series of population size
population_size_1 <- deterministic_simulation(initial_state_1, projection_matrix, simulation_length)
population_size_2 <- deterministic_simulation(initial_state_2, projection_matrix, simulation_length)
# Plot
png(filename="question_25.png", width = 600, height = 400)
plot(1:length(population_size_1), population_size_1, type = "l", col = "red", xlab = "Time", ylab = "Population Size")
lines(1:length(population_size_2), population_size_2, col = "blue")
legend("topright", c("Initial conditions with 100 adults in four stages", "Initial conditions with evenly spread population in four stages"), col=c("red", "blue"), lwd=1)
Sys.sleep(0.1)
dev.off()
return("The initial distribution of the population in different life stages can affect the initial and eventual population growth.
For example, a population with a large proportion of individuals in the reproductive age range is more likely to have a higher growth rate
than a population with a smaller proportion of individuals in this age range.")
}
# Question 26
#library(stats)
trinomial <- function(pool, probs) { # Test whether all individuals are guaranteed to be assigned to the first event
if (probs[1] == 1) { # Returns a vector of all individuals assigned to the first event
return(c(pool, 0, 0))
}
# Using the binomial distribution to determine the number of first events
first_event = rbinom(1, pool, probs[1])
# Calculate the number of individuals remaining in the pool
remaining_pool = pool - first_event
# Calculate the conditional probability of the second event if the first event does not occur
prob_event_2 = probs[2] / (1 - probs[1])
# Use binomial distribution to determine how many individuals are assigned to the second event
second_event = rbinom(1, remaining_pool, prob_event_2)
# Number of third events
third_event = remaining_pool - second_event
return(c(first_event, second_event, third_event))
}
# Question 27
# state: a vector of population states representing the number of individuals at each life stage
# projection_matrix: a matrix containing the probability of survival and maturity at each life stage
# It returns a new population state vector representing the number of individuals at each life stage after random application of survival and maturity
survival_maturation <- function(state, projection_matrix) { # Initialize the new population state vector with 0
new_state <- rep(0, length(state))
new_additions_to_stage_i <- rep(0, length(state))
# 'for' loop of all life stages except the final stage
for (i in 1:(length(state)-1)) {
# Find the number of individuals in the current stage
current_stage_pop <- state[i]
# Generation of the number of individuals to remain in the current stage and transition to the next stage
N_total <- trinomial(current_stage_pop, c(projection_matrix[i, i], projection_matrix[i+1, i]))
N_stay <- N_total[1]
N_mature <- N_total[2]
new_additions_to_stage_i[i+1]<- N_mature
#    print(new_additions_to_stage_i)
new_state[i] <- N_stay + new_additions_to_stage_i[i]
}
# Generate the number of individuals that survived the final life stage
# add these individuals to the new_state
new_state[length(state)] <- rbinom(1, state[length(state)], projection_matrix[length(state), length(state)]) + new_additions_to_stage_i[length(state)]
return(new_state)
}
# Question 28
random_draw <- function(probability_distribution) {
sample <- runif(1, min = 0, max = 1)
cumulative_distribution <- cumsum(probability_distribution)
# Determine what is the cumulative probability corresponding to the sample
for (i in 1:length(cumulative_distribution)) {
if (sample <= cumulative_distribution[i]) {
return(i)
}
}
}
# Question 29
stochastic_recruitment <- function(projection_matrix, clutch_distribution) {
ncol <- ncol(projection_matrix)
recruitment_rate <- projection_matrix[1, ncol]
clutch_size <- length(clutch_distribution)
# Calculate clutch_size expectations
expected_clutch_size <- sum(clutch_distribution * 1:clutch_size)
# Calculate recruitment_probability
recruitment_probability <- recruitment_rate / expected_clutch_size
return(recruitment_probability)
}
# Question 30
offspring_calc <- function(state, clutch_distribution, recruitment_probability) {
adults <- state[length(state)] # the number of adults
clutches <- rbinom(1, adults, recruitment_probability) # the number of clutches
#  clutches
# initialized offspring
total_offspring <- 0
if (clutches > 0) {
for (i in 1:clutches) {
clutch_size <- random_draw(clutch_distribution) # clutch size from distribution
total_offspring <- total_offspring + clutch_size # add clutch size into total number
}
}
return(total_offspring)
}
# Question 31
stochastic_step <- function(state, projection_matrix, clutch_distribution, recruitment_probability) {
new_state <- survival_maturation(state, projection_matrix)
#  print(new_state)
num_offspring <- offspring_calc(state, clutch_distribution, recruitment_probability)
#  print(num_offspring)
# Calculate the number of offspring
new_state[1] <- new_state[1] + num_offspring
# Add offspring to the first stage
return(new_state)
}
# Question 32
stochastic_simulation <- function(initial_state, projection_matrix, clutch_distribution, simulation_length) {
population_size = rep(0, simulation_length +1) # preallocation
population_size[1] <- sum(initial_state) # Adding the initial state to the population size vector
# Calculate recruitment probability
recruitment_probability <- stochastic_recruitment(projection_matrix, clutch_distribution)
for (i in 1:simulation_length) {
#  If the population size is 0, the simulation stops
if (population_size[i] == 0) {
return(population_size)
}
# Randomly sampling
new_state <- stochastic_step(initial_state, projection_matrix, clutch_distribution, recruitment_probability)
initial_state <- new_state
# state <- population_size[i] #??
population_size[i+1] <- sum(new_state)
# Adding a new state to the population size vector
}
return(population_size)
}
# Question 33
question_33 <- function() {
simulation_length <- 24
clutch_distribution <- c(0.06,0.08,0.13,0.15,0.16,0.18,0.15,0.06,0.03)
projection_matrix <- matrix(c(0.1, 0.6, 0.0, 0.0,
0.0, 0.4, 0.4, 0.0,
0.0, 0.0, 0.7, 0.25,
2.6, 0.0, 0.0, 0.4),nrow=4,ncol=4)
initial_state_1 <- state_initialise_adult(4, 100)
initial_state_2 <- state_initialise_spread(4, 100)
population_size_1 <- stochastic_simulation(initial_state_1, projection_matrix, clutch_distribution, simulation_length)
population_size_2 <- stochastic_simulation(initial_state_2, projection_matrix, clutch_distribution, simulation_length)
# Plot
png(filename="question_33.png", width = 600, height = 400)
Sys.sleep(0.1)
plot(1:length(population_size_1), population_size_1, type = "l", col = "red", xlab = "Time", ylab = "Population Size",ylim = c(0, 100000))
lines(1:length(population_size_2), population_size_2, col = "blue")
legend("topright", c("Initial conditions with 100 adults in four stages", "Initial conditions with evenly spread population in four stages"), col=c("red", "blue"), lwd=1)
dev.off()
return("The earlier deterministic simulations were smooth, while the stochastic simulations are more jagged.
This is because the deterministic simulations assumed a fixed number of clutches each time step,
while the stochastic simulations randomly selected the number of clutches from a given distribution.")
}
question_33()
# Question 29
stochastic_recruitment <- function(projection_matrix, clutch_distribution) {
ncol <- ncol(projection_matrix)
recruitment_rate <- projection_matrix[1, ncol]
clutch_size <- length(clutch_distribution)
# Calculate clutch_size expectations
expected_clutch_size <- sum(clutch_distribution * (1:clutch_size))
# Calculate recruitment_probability
recruitment_probability <- recruitment_rate / expected_clutch_size
return(recruitment_probability)
}
question_33()
# Question 32
stochastic_simulation <- function(initial_state, projection_matrix, clutch_distribution, simulation_length) {
population_size <- rep(0, simulation_length+1)
population_size[1] <- sum(initial_state) # Adding the initial state to the population size vector
population_size
# Calculte recruitment probability
recruitment_probability <- stochastic_recruitment(projection_matrix, clutch_distribution)
recruitment_probability
for (i in 1:simulation_length) {
# If the population size is 0, the simulation stops
if (population_size[i] == 0) {
# Fill the 0 vector
#for (j in (i+1):simulation_length) {
#population_size[j] <- 0
#}
break
}
# Random Sampling
new_state <- stochastic_step(initial_state, projection_matrix, clutch_distribution, recruitment_probability)
initial_state <- new_state
population_size[i+1] <- sum(new_state)
# Adding a new state to the population size vector
}
return(population_size)
}
# Question 33
question_33 <- function() {
simulation_length <- 24
clutch_distribution <- c(0.06,0.08,0.13,0.15,0.16,0.18,0.15,0.06,0.03)
projection_matrix <- matrix(c(0.1, 0.6, 0.0, 0.0,
0.0, 0.4, 0.4, 0.0,
0.0, 0.0, 0.7, 0.25,
2.6, 0.0, 0.0, 0.4),nrow=4,ncol=4)
initial_state_1 <- state_initialise_adult(4, 100)
initial_state_2 <- state_initialise_spread(4, 100)
population_size_1 <- stochastic_simulation(initial_state_1, projection_matrix, clutch_distribution, simulation_length)
population_size_2 <- stochastic_simulation(initial_state_2, projection_matrix, clutch_distribution, simulation_length)
# Plot
png(filename="question_33.png", width = 600, height = 400)
Sys.sleep(0.1)
plot(1:length(population_size_1), population_size_1, type = "l", col = "red", xlab = "Time", ylab = "Population Size",ylim = c(0, 100000))
lines(1:length(population_size_2), population_size_2, col = "blue")
legend("topright", c("Initial conditions with 100 adults in four stages", "Initial conditions with evenly spread population in four stages"), col=c("red", "blue"), lwd=1)
dev.off()
return("The earlier deterministic simulations were smooth, while the stochastic simulations are more jagged.
This is because the deterministic simulations assumed a fixed number of clutches each time step,
while the stochastic simulations randomly selected the number of clutches from a given distribution.")
}
question_33()
# Question 33
question_33 <- function() {
simulation_length <- 24
clutch_distribution <- c(0.06,0.08,0.13,0.15,0.16,0.18,0.15,0.06,0.03)
projection_matrix <- matrix(c(0.1, 0.6, 0.0, 0.0,
0.0, 0.4, 0.4, 0.0,
0.0, 0.0, 0.7, 0.25,
2.6, 0.0, 0.0, 0.4),nrow=4,ncol=4)
initial_state_1 <- c(0, 0, 0, 100)
initial_state_2 <- c(25, 25, 25, 25)
population_size_1 <- stochastic_simulation(initial_state_1, projection_matrix, clutch_distribution, simulation_length)
population_size_1
population_size_2 <- stochastic_simulation(initial_state_2, projection_matrix, clutch_distribution, simulation_length)
population_size_2
# Plot
png(filename="question_33.png", width = 600, height = 400)
plot(1:length(population_size_1), population_size_1, type = "l", col = "red", xlab = "Time", ylab = "Population Size",ylim = c(0, 1000))
lines(1:length(population_size_2), population_size_2, col = "blue")
legend("topright", c("Initial conditions 1", "Initial conditions 2"), col=c("red", "blue"), lwd=1)
Sys.sleep(0.1)
dev.off()
return("The earlier deterministic simulations were smooth, while the stochastic simulations are more jagged.
This is because the deterministic simulations assumed a fixed number of clutches each time step,
while the stochastic simulations randomly selected the number of clutches from a given distribution.")
}
question_33()
